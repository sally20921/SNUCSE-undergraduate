수업 필기
========

> 9월 1일

안녕하세요.

강의실이 모자라네? 드랍할사람 많을테니까.. 빨리 드랍하세요.

오늘 해보고, 다음시간까지 해보고 강의실 진짜 필요하면 옮길게요.

요즘 머신러닝이 안쓰이는데가 없어서, 요번학기도 전공이 다양하네. 컴공이 많긴하고
기계항공, 전기전보, 과학생물, 산업공학 다있어요. 자전도있어. 여기 뇌와 계산으로
신청한사람? 다른사람은요? 없어요? 원래 연합전공에 뇌 마음 프로그래밍 이런게
있는데 합반하기로했어요.

머신러닝이, 사실은 AI가. 인공지능이 역사가 오래돼요. 머신러닝을 바라보는 관점이
여럿이 있어요. 통계학, 계산이론, 등등. 근데 아직은 인공지능으로 바라보는 시각이
강한것같아요. 머신러닝이 응용폭이 굉장히 커요. 반도체공정에서 생기는 데이터도
머신러닝으로 분석하고. 삼성전자에서도 강의를 여러번 했어요. 다들 알다시피
인터넷이나 쇼핑몰같은곳에서도 비즈니스에 직접 쓸 수 있고, 구글 페이스북 갈라면
머신러닝을 해야될거에요.

공고가 된 실라부스가 있는데, 내가 강의자료를 다시 정리하려고 생각하고있어서,
전체적인 내용은 비슷할텐데 순서가 좀 달라질것같아요. 오늘 할 일은, 이 과목을
계속 들을거냐 에 도움을 주면 될것같아요. 어떤 내용을 배울지 개괄적으로 한시간만
수업할게요.

평가방식 이런거 먼저 얘기를 할까?

* 오픈북 시험 두번 (50%)
* 미니 프로젝트 두번, 포스터 1회 (40%)

뉴럴넷을 직접 써볼건데 한번은 감도학습, 한번은 self-organizing map이라고 잘
안쓰는거지만 해볼거에요. 프로그래밍은 직접 안해도 돼. 이미 있는 툴을 그냥
쓸거에요.

학기 말에 각자가 한 프로젝트를 학회 하듯이 발표를 해. 보고서도 간단히 내는데,
포스터를 벽에 붙이고 발표를 할거에요.

내 과목이 좀 빡빡해요. 빡빡한데, 끝까지 버티면 성적이 나쁜건 아냐. 그러니 빨리
도망갈사람 도망가시고, 버티는 사람들이 약간 절대평가 성격으로 갑니다.

##### 뉴럴넷 학습시킬때 데이터셋 정해주시나요?
정해줍니다. 어느정도까진 보통 하는걸로하고, 나머지는 열어둡니다.

##### 프로젝트 주제는 완전히 자유인가요?
아뇨 꼭 그렇지는 않아요. 교육이 목적이니까 툴을 명확하게 하나로 정하고 데이터는
반반정도.

머신러닝이 실험할때 파라미터들이 좀 있어요. 틀을 정해줘도, 파라미터를 다양하게
바꿔가면서 할 수 있어요. 이과목만 가지고 여러분들이 시간을 다 쓸 수 없다는건
알긴 하는데, 여러분이 이거에 시간을 좀 쓸 생각을 하셔야해요.

이번학기에 크게 하는게 감독, 무감독 학습이에요. K-means, 클러스터링을 공부할거고
프로젝트 두번째로 할것이 Self-organizing map, 그 다음이 베이지안 네트워크. 그
다음이 하이퍼넷.

그다음 내가 까먹기전에 공지할게 있어요. 다음주에 내가 일이 있어서, 보강을 미리
할게요.

*월요일 19~22시*

여기 비디오를 하나 볼게요. 소리가 안나네용. 마이크를 설치해야하는구나. 그냥 보세요.

일반적인 것들은 입력이 들어가면, 알고리즘을 거치면 결과로 데이터가 나와요.

하지만 머신러닝은 데이터가 들어가면 머신러닝을 통해 알고리즘이 결과로 나와요.

시행착오를 반복하고, 경험으로부터 스스로를 향상시켜요.

학습에도 여러가지 종류가 있어요

- 감독학습
- 무감독학습
- 강화학습

강화학습은 잘하면 당근을 주고 못하면 채찍을 주는거에요 찰싹찰싹!  동물을 훈련하듯이.

그리고 감독학습은 우리가 많이 하는건데 정답을 알려주는거에요. 예를들어 문자인식.
직선은 1이다, 이건 2다, 이걸 다 알려주고 배우게 시키는거에요.무감독 학습은

데이터만 주고 아무것도 안해. 기계가 알아서 스스로 패턴을 찾도록 하는거에요.

숫자를 다 섞어놓고 온갓 다 섞어놓고 무감독학습 알고리즘을 돌리면 1자는 1자끼리
묶어주고 2는 2끼리 묶어주고 이런 현상이 발생해요.

#### 기계학습의 정의
환경 E 아래에서, 데이터 D로부터 모델 M을 만드는 알고리즘. 성능을 평가하는 P가
있어요.

#### 왜 기계학습인가?
* 프로그래밍이 어여룬 문제들
  * No explicit knowledge
  * 사람이 직접 코딩해주기 힘듬
  * 환경이 계속 변함
* IT 환경의 변화
  * 빅데이터
  * 컴퓨터 파워
  * 모바일 서비스
* 직접적인 비즈니스 가치 창출
  * 유저 특화
  * 광고
  * 추천

내가 박사때만해도 머신러닝 하는사람 별로 없었는데, 20~30년이 지나는 사이에
기초연구에서 산업이 되었어. 왜 그러느냐? 빅데이터 덕분이야.

컴퓨팅파워, 20년전에도 모자라고 지금도 모자라. 메모리도 모자라. 20년 후에도
모자랄거야.

#### 기계학습의 역사
(피피티 참고)

20년전엔 속도가 달려서 거의 SVM을 써왔음.

역사가 1950년부터 2005년까진 한산한데, 2005년부터 2015년까지 엄청 빽뺵함.

**2012: Deep learning breakthroughs (Google, MS)**

2015: 일론 머스크가 유용하고 안전한 AI를 만들라고 100억을 도네이션.

DARPA 무인차 경주대회. 2014년에도 했는데 단 한팀도 완주를 못했다가, 2015년엔
모두가 성공. 스탠포드 AI랩이 우승했음. 세바스찬, 나랑 같이 공부했던사람.
이게 경쟁은 커녕, 10시간동안 완주하는것도 힘듬.
1등을 하려다보니, 추월도 하고 그럼.

시리. 처음엔 머신러닝 기술이 들어갔지만, 머신러닝을 쓰진 않았는데 이젠
머신러닝을 쓰고있음.

2007년, IBM 왓슨. 위키피디아랑 웹에 있는 텍스트를 싹다 학습시킴.

#### Human-Level Face Recognition
페이스북 얼굴인식하는 뉴럴넷. Deep-face

100만개의 이미지의 클래스를 나누는 AI. 구글.

#### 음성인식 (Acoustic Modeling)
지금까진 음성 노이즈 제거하고, 자꾸 뭐하고, 뭐하고 엔지니어링으로 해결해왔는데

이젠 그냥 raw 데이터를 다 때려박아서 학습시켰더니 훨씬 잘나옴.

#### Movie Recommendations in Netflix
넷플릭스랑 아마존에서 오래전에 상금을 걸고 알고리즘 컴피티션을 열고 그랬음.

> 9월 3일

```
@김젼	출석을 다부르기엔
@김젼	우ㅇ으으으읔
@김젼	출석을 빨리 한번만 불러볼게용
@sgk	중계부탁
@김젼	빨리부릅니다
@김젼	이태석
@김젼	박상률
@김젼	sgm
@김젼	(?)
@sgk	그런거말고
@김젼	다음시간엔 강의실 바꾼대요
@김젼	박성원
@김젼	예에!
@김젼	오늘은 여기까지만 부를게요
@김젼	아래에 107호 비어있는데 옮길까요?
@김젼	네
@김젼	sgk: 오셈
@sgk	옮김?
@sgk	수업자료를 보고 결정해야지
@Gallen	에구 우리애
@Gallen	들으려고 했던 피엘 수업이 팀플이 있나본데
@Gallen	도강하는 입장에서 팀플은 좀 ㅁㄴㅇㄹ이라
@Gallen	그냥 나랑 책 보고 공부하자고 했는데
@sgk	오늘은 그냥 중계만 들어도 되는 수준같으니
@sgk	다음시간부터 열심히 들어야지
@Gallen	SICP 번역본 보면 적당하려나
@김젼	자
@뀽	마의 강의록
@김젼	이 강의실이 딱 맞네
@Gallen	zㅏ
@김젼	오늘의 주제는
@뀽	마의 렉처노트 보세요
@김젼	머신러닝이 무엇이냐를
@김젼	formal하게 다뤄볼거에요
@김젼	....
@sgk	sicp 번역본 추천
@Gallen	저도 서른살이 되기 전에는 그걸 다 읽어보려고 하긴 했거든요
@sgk	역시 사람이 pl을 공부하려면 리습을 써야지
@Gallen	이참에 같이 알려주면서
@Gallen	보는게
@Gallen	헉 알려준대 내 주제에..!
@Gallen	흠흠
@김젼	머신러닝을 크게보면
@김젼	Function approximation
@김젼	또 하나는
@Gallen	어제는 오일러를 한 7~8번까지 풀게 시켰는데
@김젼	펑션 어프록시메이션이
@김젼	약간 결정적인 모델이고
@김젼	반대편 하나는
@김젼	Probability estimation
@김젼	이 두가지를
@Gallen	제가 '오일러'를 풀라고 시킨 건 아니고 어디서 누가 추천을 해줬다길래 봐줬는데
@김젼	이야기해보겠어요
@김젼	하나는 함수근사
@김젼	또 하나는 확률문제
@Gallen	오일러가 프로그램을 더 잘? 예쁘고 똑똑하게? 짜는데에는 별로 도움이 안되는 거 같아서
@김젼	이 두개의 관계를 이해해야돼요
@김젼	무슨 머신러닝 알고리즘이 나와도
@Gallen	앗 필기 다 하면 말해야지
@김젼	이 두개를 알지 않고
@김젼	넘어가기 어려워요
@김젼	>ㅅㅇ
@김젼	글자를 써요
@김젼	2
@김젼	2
@김젼	(서로 다른모양의 2들)
@김젼	2
@김젼	2
@김젼	홍진호
@김젼	콩
@김젼	2
@김젼	그리고 여기에다가는 3을 써요
@김젼	3
@김젼	3
@김젼	등등등등
@김젼	데이터가 주어져요
@김젼	머신러닝이라는거는
@김젼	이 많은 데이터
@김젼	답을 가르쳐줬죠?
@김젼	문제 - 답, 문제 - 답
@김젼	이 엄청나게 많은 문제 - 답 쌍을 만들어놓고는
@김젼	내가 새로 문제를 하나 딷ㄱ 내면
@김젼	이거 답이 뭐냐
@김젼	이거를 맞추는거에요
@김젼	전형적인 패턴인식이죠?
@김젼	스마트폰에서
@김젼	필기인식 해봤으면 알겠지만
@김젼	아직 완전하지 않아요.
@김젼	숫자는 잘 돼요
@김젼	근데 한글, 일반적인 글자 등 하며는
@김젼	완전하진 ㅇ낳아요
@김젼	이게 감독학습? 무감독학습?
@김젼	(감독학습이용)
@김젼	지금 여기엔
@김젼	문제들에 레이블이 달려있는데
@김젼	레이블이 없으면
@김젼	무감독학습.
@김젼	그러면 데이터를 어떻게 표현하느냐
@김젼	보통 우리가 계속
@김젼	머신러닝에서 계속 생각할 식은
@김젼	이거에요
@김젼	답이
@김젼	X 벡터라고 생각하자
@김젼	이 그림이
@김젼	이 그림을
@김젼	비트맵으로 표현해요
@김젼	(0, 0, 0, 0, ....... 1, 0) <- 이게 문자 그림
@김젼	X = (0, 0, 0, 0, ....... 1, 0) <- 이게 문자 그림
@김젼	Y는
@김젼	문제의 답.
@김젼	이 그림과 답을
@김젼	어떻게 X, Y로 인코딩하느냐도
@김젼	중요한 문제에요
@김젼	인코딩하는 방법에 따라 답이 잘 나올수도 있고 아닐수도있어요
@김젼	보통은 벡타형태로 표편해요
@김젼	0부터 9까지의 숫자를 어떻게 벡터로 표현할까?
@김젼	1 of N 인코딩을 쓰자
@김젼	만약에 익
@김젼	이게
@김젼	2다! 하면
@김젼	(0, 0, 1, 0, 0, 0, 0, 0, 0, 0)
@김젼	0, 1, 3~9일 확률은
@김젼	0이고
@김젼	2일 확률이
@김젼	100%
@김젼	이걸 1-of-n 노테이션이라고 그래요
@김젼	트레이닝 데이터는
@김젼	이러한 X와 Y들의 순서쌍 (X,Y)e들의 집합이에요
@김젼	트레이닝 데이터 D = {(Xi, Yi) | i = 1, ..., N }
@김젼	 N 은 데이터의 수
@김젼	90년대 초반부터
@김젼	이러한 OCR 트레이닝 데이터를 꾸준히 모아오고
@김젼	컴피티션을 하고 그랬어요
@김젼	내가 공부할때엔 94~95% 성공률이었어요
@김젼	많은 AI 문제가
@김젼	성능이
@김젼	100%에 가까워질수록
@김젼	1% 성능을 향상시키기가
@김젼	점점점 힘들어져요
@김젼	90%까지는
@김젼	금방 가
@김젼	근데 90%에서 95%로 가기는 정말 힘들고
@김젼	95%에서 97.5%로 가기는 더더욱 힘들어요
@김젼	갈수록 상당한 시간이 드는데
@김젼	이 시간이라는게 1~2년이 아니고
@김젼	1950년대에
@김젼	AI 기술이 생겼을떄
@김젼	그당시 사람들은
@김젼	1980년대면 컴퓨터가 사람이랑 말을 자유롭게 할거라고 생각했어요
@김젼	근데 2015년에 와서도 잘 안되죠?
@김젼	그래요
@김젼	온르은
@김젼	오늘은
@김젼	감독학습 위주로
@김젼	이야기를 할게요
@김젼	요거를 뽀말하게 표시를 하면
@김젼	Xi = (x1, x2, x3, ..., xn)
@김젼	Yi = (y1, y2, .., ym)
@김젼	인풋(X)은 n차원의 바이너리, 실수 벡터
@김젼	그리고
@김젼	내가 에러를 정의해요
@sgk	이상하다
@sgk	수업내용이 ppt랑 다르다
@김젼	E = Sigma | yd - f(xd * w) |
@김젼	PPT 안봄
@sgk	뭑
@김젼	E = Sigma | yd - f(xd * w) |^2   (d = 1 ~ N)
@김젼	f(xi * w) 가 뭔지
@김젼	설명해드릴게요
@김젼	그냥 샘플하는 함수에요
@김젼	f는
@김젼	결과로 y가 나오는거에요
@김젼	w는 뭐냐면
@김젼	f는 모델
@김젼	학습이 뭐라그랬어요
@김젼	데이터로부터 모델 만드는거라그랬어요
@김젼	D => M
@김젼	w가 모델이 쓰는 파라미터에요
@김젼	f는 모델이고.
@김젼	예를들어
@김젼	f가 뉴럴넷이다 치면
@김젼	뉴럴넷 사이에서 쓰는 역전파속도 등등등 모오든 파라미터가 w라고 생각하면 돼요
@김젼	f가 디시젼트리다 치면
@김젼	w는 좔좔좔좔이에요
@김젼	그래서
@김젼	E = Sigma | yd - f(xd * w) |^2   (d = 1 ~ N)
@김젼	이게 아주 제너럴한 정의에요
@김젼	가장 심플한 정의에요
@김젼	원하는 목표치가 yd
@김젼	실제 출력치가 f(xd * w)
@김젼	이거의 차이를
@김젼	제곱합한것이
@김젼	에러인거에요
@김젼	문자인식으로 돌아가면
@김젼	내가 뉴럴넷을 만들어서 트레이닝을 열심히 했어
@김젼	보통
@김젼	문자인식을 하면
@김젼	얘가 1이랑 얼마나 닮았는지, 2랑 얼마나 닮았는지 정도가 실수로 나와요
@김젼	이 숫자가 2면 내가 원하는값은
@김젼	(0, 0, 1, 0, 0, ~) 인데
@김젼	실제로 나오는 값은
@김젼	(0.1, 0.1, 0.9, 0.1, ~~) 이럴거 아냐
@김젼	이거를 빼서 제곱합하면
@김젼	에러가 되는거징
@김젼	학습이라는건 뭐냐면
@김젼	이러한 상황에서
@김젼	Find f(-;w) such that y = f(x;w)
@김젼	            or minimizing E_N
@김젼	이게 학습의 정의에요.
@김젼	                   Given D
@김젼	y = f(x;w)
@김젼	정확하게 값이 같아지는
@김젼	f w 를 찾는게
@김젼	머신러닝의 꿈이에요
@김젼	이건 이상이고
@김젼	그럴 수 없으니
@김젼	E를 최소화하는거에요
@김젼	이것이 바로
@김젼	Function approximation이에요.
@김젼	아주 쉬운말로 생각을 해보자
@wook	btzhang 수업?
@김젼	네
@김젼	그래프를 하나 그리고
@김젼	가로축이 애긔들의 나이
@김젼	한살 두샬 세샬
@김젼	세로축이
@김젼	애긔들의 키
@김젼	라고 생각해보자
@김젼	데이터포인트가
@wook	한글수업인가보네
@김젼	이렇게 뚝뚝뚝 주어져있어요
@김젼	예를들어
@김젼	두살짜리 애가
@김젼	몇센치지?
@김젼	한 70센치라고 쳐봐
@김젼	아니다 50
@김젼	그리고
@김젼	다섯살짜리애가 80센치
@김젼	이랬다고 쳐보자
@김젼	이게 데이터셋이죠? (그래프 위에 무수한 점들)
@김젼	x1 = (2yr, 50cm)
@김젼	x2 = (3yr, 55cm)
@김젼	x3 = (2yr, 52cm)
@김젼	이걸
@김젼	숫자로 그대로 써도 되지만
@김젼	우리가 이해하기엔
@김젼	그냥 이렇게 2차원짜리 인티저로
@김젼	쓰는게 쉽죠?
@김젼	근데 바이너리 벡터로 펼치는게
@김젼	머신러닝이 더 나아요
@김젼	그게 사실
@김젼	지금
@김젼	머신러닝 뉴럴넷들을
@김젼	사람들이
@김젼	그냥 블랙박스처럼 쓰잖아요?
@김젼	툴 주면 딱 쓰고
@김젼	근데 내가
@김젼	머신러닝을 알아야 한다는 말이
@김젼	이런 입출력을
@김젼	어떻게 인코딩할건지
@김젼	그런 노하우를 알아야돼요
@김젼	아 미아
@김젼	미안
@김젼	저거 저렇게쓰는거 아냐
@김젼	(x1, y1) = (2yr, 50cm)
@김젼	(x2, y2) = (3yr, 55cm)
@김젼	고마워요
@김젼	제대로 지적했어요
@김젼	머신러닝이라는건
@김젼	x가 주어지면
@김젼	y를 찾는 문제에요
@김젼	그래서 내가
@김젼	이 점들사이에
@김젼	추세선을 그렸다고 쳐봐
@김젼	데이터가 없는데
@김젼	선을 쭉 그었어
@김젼	쮸욱
@김젼	그럼 예측을
@김젼	요값으로 예측할 수 있죠?
@김젼	이게 바로 예측이에요
@김젼	이거
@김젼	전혀 새로운 문제가 아니죠
@김젼	고등학교떄부터 한거에요
@김젼	뻥셔널 인터폴레이션
@김젼	인터폴레이션이 한글로 뭐지?
@김젼	나: 보간이요
@김젼	보간
@김젼	이게 수학에선
@김젼	몇백년된 문제죠?
@김젼	함수를
@김젼	근사하는거에요
@김젼	머신러닝은
@김젼	아주 복잡한 모양의
@김젼	이 임의의 함수를
@김젼	근사하는거에요
@김젼	수학에서는
@김젼	다항식을 쓰지?
@김젼	폴리노미얼 인터폴레이션 하면서
@김젼	1차 다항식으로 근사할수도있고
@김젼	3차다항식으로 근사할수도 있고
@김젼	여튼 공부를 하다보면
@김젼	이런 뻥셔널 어프록시메이션이 종종 나올거에요
@김젼	명확하죠?
@김젼	머신러닝이 뭘 하려고하는건지
@김젼	근데 요렇게만 설명하면
@김젼	머신러닝을 왜 해야하는지
@김젼	이해가 안되겠죠
@김젼	이거
@김젼	수치해석에서 많이 나온 테크닉이에요
@김젼	그래서
@김젼	지금 머신러닝이
@김젼	과거에 있던 통계학이나 수학을
@김젼	쓰기도 해요
@김젼	근데 이걸로 끝나지 않아요
@김젼	이건 아주 작은 시작이죠
@김젼	예를들어 수학은
@김젼	이런걸 아주 간단하게만 어프록시메이션할라그랬는데
@김젼	딥러닝에선
@김젼	굉장히 복잡한 모양의 커브를 만들지
@김젼	여튼 요게 뻥셔널 어프록시메이션으로 본 감독 학습이었어요
@김젼	무감독 학습은 모냐!
@김젼	쓱싹쓱싹
@김젼	무감독 학습은
@김젼	똑같은 데이타아
@김젼	어 그니까
@김젼	어 음
@김젼	데이타
@김젼	요거를 좀 지우고할게요
@김젼	무감독 학습을
@김젼	뻥셔널 어프록시메이션으로 본다면
@김젼	데이터가 저런식으로 안주어지고
@김젼	이렇게 주어져요
@김젼	y가 없어
@김젼	D = { xd | d = 1~N }
@김젼	E는 어떻게 정의하냐면
@김젼	E_N = Sigma | xd - f(xd;w) |^2
@wook	오토인코더!
@김젼	     = Sigma SIgma ( xk(d) - fk(xd;w) )^2
@김젼	타자로 못치겠졍
@김젼	f는
@김젼	f :: Vector
@김젼	f = (f1, f2, f3, ... ) 이렇게
@김젼	f도 출력값이 벡터라서
@김젼	차원에 따라 잘라서
@김젼	f1 f2 이렇게 표현할 수 있거든요?
@김젼	음
@김젼	벡터합
@김젼	제곱한걸 굳이 설명안해주셔도
@김젼	될거같은데
@김젼	*벡터차 제곱한거
@김젼	무감독 학습은 y가 없어용?
@김젼	에라함수를 어떻게 정의하냐면
@김젼	입력을 줬쬬?
@김젼	입력하고 출력이 같아요
@김젼	내가 입력주고 f라는 함수를 만들고 출력을 바라는데
@김젼	출력이 입력하고 똑같길 바래
@김젼	이걸 그냥 카피해버리면 쉽죠?
@김젼	근데 학습이 안일어나지
@김젼	그래서 f를 만들어내야돼
@김젼	그래서 무감독학습이란 뭐냐면
@김젼	Find f(-;x) such that x = f(x;w)
@김젼	             or inimizing E_N, Given D
@김젼	그래서
@김젼	애니를 열심히 학습시켜서
@김젼	애니를 안볼때에도 덕질을 하는 AI를 만드는거시다
@김젼	뽀롱뽀롱뽀로로
@김젼	이 에라를 줄이는것이
@김젼	머신러닝이 하는일이야
@wook	무감독학습의 한 분류로 저런 걸 하는건 맞는데 무감독학습은 그게 다인것처럼 이해될 우려가 ㅠㅠ
@김젼	머신러닝의 분류는
@김젼	f를 어떻게 만드느냐
@김젼	로 분류할 수 있고
@김젼	f하고 W의 모양이 어떻게 되느냐
@김젼	도 많이 달라져
@김젼	또 이제 할얘기는
@김젼	감독학습과 무감독학습의
@김젼	관계에요
@김젼	이거 아주 중요해용
@김젼	이 둘은
@김젼	아주 밀접한 관계가 있어요
@김젼	이미 눈치챘겠지만 ㅇㅅㅇ)+
@김젼	감독 vs 무감독
@wook	ㅋㅋ
@김젼	내가
@김젼	z라는 벡터를 만들게요
@김젼	z = (x, y)
@김젼	그러면 트뢰이닝 데이타가 어떻게 주어지냐면
@김젼	요번엔 z라는걸로 묶어서 생각하니까
@김젼	D = { Zd = (Xd, Yd) | d = 1~N }
@김젼	아 그리고 지금
@김젼	제곱합 계속 쓰니까
@김젼	제곱합 연산자좀 정의할께요
@김젼	이거 타이핑 어떻게해
@김젼	ㅇㅅㅇ
@김젼	님들 빨리
@김젼	제곱합함수
@p	아으 죽겠네
@김젼	아이디어좀
@p	제곱합함수?
@wook	..?
@김젼	sum of squared error
@wook	SSE?
@p	sigma a_n^2 ?
@p	아하
@김젼	우왕
@p	sse로 합시다
@김젼	E = SSE(Z - f(z;w))
@김젼	요렇게 되겠죠?
@김젼	고다음메
@p	streaming simd extensions
@김젼	sse
@김젼	Find f(-;w) such that z = f(z;w)
@김젼	        or minimizing E_N, Given D
@김젼	자 이렇게 감독학습을 정의해보아써요
@김젼	자네 펜하나만 좀 빌려주라
@김젼	자네 이름이 머였지
@김젼	(뭔가 적음)
@김젼	수업노트 고치시는듯
@김젼	자 이렇게
@김젼	뽀뮬레이션하면
@김젼	감독학습이
@김젼	무감독학습으로 변신했죠?
@김젼	그냥 x대신 z로 바꾼거자나
@김젼	ㅇㅅㅇ+
@김젼	그냥 z를
@김젼	입출력을 합친
@김젼	(X, Y)로 만들고
@김젼	무감독학습으로 만들었찌
@김젼	자 이렇게
@김젼	감독학습과 무감독학습, 무감독학습모양으로 만든 감독학습
@김젼	세개를 보자
@김젼	뽀롱뽀롱
@김젼	뽀로로로롱
@김젼	크롱
@김젼	이
@김젼	내가 제일
@김젼	맨 마지막에 보여준게
@김젼	유니버셜 레프리젠테이션이에요
@김젼	감독 무감독을 나눌 필요도 읎어
@김젼	주어진 데이터를
@김젼	리컨스트럭션한 알고리즘을
@김젼	만들면
@김젼	이건 이론적으로 유니버셜해요
@김젼	실제로 잘하느냐와는 달라
@김젼	이론적으로 범용적이면 프랙티컬하게는 범용성이 큰경우가 많아
@김젼	여튼 이 맨 왼쪽 포뮬레이션으로 만들면 유니버셜하고
@김젼	내가 이론적으로도 권장해요
@김젼	그리고 내가 한학기동안 가르칠게
@김젼	이거에요
@김젼	*맨 오른쪽 포뮬레이션
@김젼	f(z;w)
@김젼	지금 우리가
@김젼	블랙박스를 생각해보면
@김젼	f 를
@김젼	x는 인풋을 넣으면
@김젼	y가 나오는걸 생각했죠?
@김젼	y = f(x;w) :: 감독학습
@김젼	무감독학습을
@김젼	생각하면
@김젼	x하고 y를 다 넣고
@김젼	x하고 y가 나오길 기대하는거에요
@김젼	z = f(z;w)
@김젼	근데 여기서
@김젼	y를 생각하는게
@김젼	필요는 한게
@김젼	트레이닝할때엔
@김젼	x,y를 붙여서 하겠지만
@김젼	우리가 실제로 써먹을때엔
@김젼	y가 없고 x만 있잖아?
@김젼	얘가
@김젼	내부적으로
@김젼	조인트 프로바빌리티 (뭔지모름)이 있어서
@김젼	x없이 y를 만들어낼 수 있고
@김젼	거꾸로 y만 주고
@김젼	x를 만들어낼 수 있어요
@김젼	우리가 기계를 상상을 못한다고 그러죠?
@김젼	근데 이제 뉴럴넷으로 학습시켜보면
@김젼	3을 상상해보세요
@김젼	(3의 이미지)
@김젼	이게 가능해지는거에요
@김젼	이론적으로 가능하고
@김젼	프랙티컬하게도 그런 시도가 있어요
@sgm	필사쟁이네
@김젼	꾸쯋
@wook	joint probability는 p(x,y) 를 의미함
@wook	p(y) = \sum_x p(x,y)
@wook	etc.
@김젼	학습과
@wook	만화이해왕
@김젼	퍼포먼스 단계를
@wook	'ㅅ'
@김젼	나눌수도있어요
@김젼	학습이
@김젼	논리게이트와 제일 다른게
@김젼	그거에요
@김젼	우리가 로직게이트는 바이너리만 입력으로 주고
@김젼	바이너리만 출력으로 나오잖아?
@김젼	0하고 1
@김젼	우리가
@김젼	학습을 할때엔
@김젼	바이너리 입력 써도 돼
@김젼	입력이 실제로 그러하다면 바이너리 입력 써도 돼
@김젼	근데 출력은 0과 1 사이의 아날로그로 나와요
@김젼	아날로그 컴퓨터를 만드려는것에 가까워요
@김젼	입력은
@김젼	이산적이지만
@김젼	출력은
@김젼	커브가 나와요
@김젼	그래서 상당히
@김젼	폴트 톨러런스하고
@김젼	어답티브해요
@김젼	로버스트해요
@김젼	주어진 데이터는 discrete한걸 쓰는데
@김젼	학습한건 임의의 노이즈가 있어도 돼요
@김젼	우리가 접하는 데이터는
@김젼	상당히 finite해요
@김젼	학습모델이 생성하는 결과는
@김젼	상당히 robust하고
@김젼	general해요.
@김젼	우리가 아직 첫시간이라서
@김젼	안한게 있는데
@김젼	내가
@김젼	f(z;w)이렇게
@김젼	미니마이즈했잖아요?
@김젼	근데 이건 좋은 답이 아니에요.
@김젼	내가 일부러 얼버무렸는데
@김젼	x가
@김젼	트레이닝 데이터 D만을 의미하는게 아냐
@김젼	인풋 스페이스에 있는
@김젼	관측하지 않은 데이터까지 포함하여
@김젼	모든 가능한 x=f(x;w) 에요
@김젼	minimizing E_N만 하면
@김젼	이건 오버피팅하게되어요
@김젼	나중에 레귤러라이제이션을 배울거에요
@김젼	우리가
@김젼	x=f(x;w)를
@김젼	카피함수로 못만드는게
@김젼	이 이유때문이에요
@김젼	학습은
@김젼	GEneralization을 하는 알고리즘이에요
@김젼	일반화해야돼
@김젼	일반화한다는게 뭐냐면
@김젼	트레이닝 데이터 N개를 학습시켰지만
@김젼	나중엔 N이 무한이 되어도.
@김젼	모든게 되어도
@김젼	E가 미니마이즈되길
@김젼	바라는거에요
@김젼	로봇축구 생각하면 쉽죠?
@김젼	축구경기장에 나가면
@김젼	온갖 상황이 벌어질텐데
@김젼	모든 상황에서도 얘가
@김젼	메시처럼 축구를 잘 하길 바라는거지
@김젼	일반화가 아주 중요한 특성이에요
@김젼	내가 수업하는거
@김젼	카메라로 찍어도 돼요
@김젼	조교가 칠판 내가 지우기 전에 좀 찍어봐요
@김젼	다른사람도 찍어도 돼용
@sgk	찍어라 김젼
@sgk	보내줘
@wook	ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ
@김젼	시렁
@wook	쿨하시네 장교수님
@김젼	초등학생 아니니까
@sgk	ㅠㅠ
@김젼	알아서들 잘 하세요
@김젼	자 그럼
@김젼	뻥셔널 어프록시메이션은 여기까지 하고
@김젼	지금까지 한 이야기들은 다
@김젼	디터미니스틱한 이야기에요
@김젼	두번쨰 정의에선
@김젼	랜덤 배리어블이 나와요
@김젼	확률모델
@p	ㅋㅋㅋㅋ
@김젼	지금 첫번째껀
@김젼	입력이같으면
@김젼	출력이 항상 같은경우였지?
@김젼	근데 두번째의 경우엔
@김젼	입력이 같아도
@김젼	출력이 달라질 수 있어
@sgm	nondeterministic 이야기 아닌가?
@김젼	그란도시즌을
@김젼	들어봐요
@김젼	하나둘셋이에요
@김젼	그란도시즌이에요?
@김젼	똑같은 입력을 줬는데
@김젼	해석이 다른거야
@김젼	달라질쑤이써
@김젼	특히 뭐
@김젼	사람이 번역한다 그러면
@김젼	맥주한잔 한거랑
@김젼	커피마시고 밤새고 한거랑
@김젼	다를수있지?
@김젼	확률모델이
@김젼	아주 흥미로운 모델인데
@김젼	믈논 장단점이 있어요
@김젼	그래서 대부분의 실제적인 모델에선
@김젼	확률모델이 잘 안쓰여
@김젼	근데 연구자들은 머신러닝에서
@김젼	확률모델을 많이 연구해왔어요
@김젼	확률모델 정의를 해볼텐데
@김젼	끝내긴 어렵고
@김젼	가다가 말게요
@김젼	자 그래서
@김젼	똑같은
@김젼	요거요거를 씁시다
@김젼	자 X와 Y라는건 알아쓰요
@김젼	감독학습을
@김젼	확률모델로 바라보면
@김젼	감독학습이 뭐냐면
@김젼	이 확률구하는 문제에요
@김젼	조건부 확률
@김젼	P(Y|X)로
@김젼	포뮬레이션할 수 있어요
@김젼	또 이걸
@김젼	Z=(X,Y)입출력을
@김젼	합쳐서 생각하면
@김젼	쪼인트 프로바빌리티 에스티메이션 하는 문제로 변해요.
@김젼	감독 : P(Y|X)
@김젼	무감독 : P(Z) = P(X,Y)
@김젼	쬬쬬
@김젼	두개의 관계는
@김젼	어떻게돼요?
@김젼	두개의 관계는 어떻게 돼요.
@김젼	그거를 알라면
@김젼	확률의 기본법칙을 한번 알아볼까용?
@김젼	확률에 기본법칙
@김젼	간단한거 두개밖에 없어요
@김젼	그냥 우리
@김젼	 초등학교때부터 배우는
@김젼	셈하는 방법
@김젼	알제브라
@김젼	배수법에
@김젼	보통 뭐배워요 우리가
@김젼	부울대수 이런거 배울떄
@김젼	가장 기본적으로 배우는게 뭐에요
@김젼	덧셈뺄셈
@김젼	확률
@김젼	곱의 법칙
@김젼	1) Product rule
@김젼	쪼인트 프로바빌리티에요
@김젼	P(X,Y) = P(X) * P(Y|X)
@김젼	       = P(Y) * P(X|Y)
@김젼	이게 곱의법칙이고
@김젼	두번째는 합의법칙이에요
@김젼	2) Sum rule
@김젼	썸룰
@김젼	썸남썸녀
@김젼	<@wook> p(y) = \sum_x p(x,y)
@김젼	정확히 같네
@김젼	이렇게 쓰면
@김젼	x가 사실 필요 없어져
@김젼	우리가
@김젼	무감독 학습을 해서
@김젼	P(x,y)를 구했어요
@김젼	동전이랑 주사위를 계속 던져서
@김젼	확률을 잘 구했어
@김젼	근데
@김젼	우리가 그냥
@김젼	주사위 눈이 6만 나올 확률이 궁금해
@김젼	그러면
@김젼	동전 앞일떄 6확률 + 동전 뒤일때 6확률
@김젼	하면 되지?
@김젼	이게 아주 뻔대맨탈한거에요
@김젼	P(Y)는
@김젼	우리가 궁금한
@김젼	알고싶은 확률이에요
@김젼	그리고
@김젼	X는
@김젼	우리 머릿속의
@김젼	숨은 변수에요
@김젼	Latent variable이라고 그래요
@김젼	자고있는 변수
@김젼	머신러닝은
@김젼	X를
@김젼	자동으로 찾는 알고리즘이지
@김젼	관측된 거대한 Y가 있는데
@김젼	모르는 뭥가가 이써
@김젼	눈에 보이지 않는 뭔가가 이써
@김젼	허블망원경으로 우리가 메져하는건 Y에요
@김젼	근데 우쥬에 무슨 숨은 법칙이 있는거같애
@김젼	그게 멀티룰 X에요
@김젼	기계의 힘을 빌려서
@김젼	모든 숨은 X의 가능성을 찾는거에요
@김젼	너무 철학적으로 얘기했나?
@김젼	근데 그게 진짜에요
@김젼	그정도로 생각하고 머신러닝을 해야돼
@김젼	그렇지 않으면
@김젼	뻥션 어프록시메이션
@김젼	이거 수치해석에 다 있는 기술이야
@김젼	이게 두개고
@김젼	3은
@김젼	앞의 두개로 유도가능한데
@김젼	중요해서 한번 더 말하자면
@김젼	베이스 룰이에요
@김젼	3) Bayes rule
@김젼	머신러닝에서 베이지안 얘기해요
@김젼	베이지안을 모르면 머신러닝을 모르는사람 취급받을거에요
@김젼	베이지안이 약간
@김젼	철학이 있어요
@김젼	통계 내에서 철학이 좀 들어가는데
@김젼	주관이 들어가
@김젼	그래서 통계학자들이
@김젼	베이지안을 최대한 배제하려고했는데
@김젼	결국 포기하고
@김젼	지금은 베이지안이 아주 널리 쓰여요
@김젼	내가 공부할때엔 베이지안이 많지는 않았어요
@김젼	아 이거
@김젼	알아
@김젼	P(Y|X) = P(X|Y) * P(Y) / P(X)
@김젼	P(X|Y) = P(Y|X) * P(X) / P(Y)
@김젼	이 베이스 룰은
@김젼	그냥 1에서 한줄이면 유도할 수 있어요
@김젼	근데
@김젼	의미는 아주 심오해요
@김젼	수식에만 사로잡혀있으면 안되고
@김젼	의미를 생각해야돼요
@김젼	P(Y|X)는
@김젼	X가 관측이고
@김젼	Y가 예측이라고 생각하면 돼
@김젼	X가 입력이고
@김젼	Y가 출력이라고 생각하면 돼
@김젼	머신러닝은 많은것이 예측 알고리즘이에요
@김젼	과거로부터
@김젼	미래를 내다보는 알고리즘이에요 다
@김젼	프로그램도 다 그런거죠?
@김젼	여러분들 짜는 프로그램 다
@김젼	과거로부터 미래를 하고싶은거에요
@김젼	근데 이 프로그램이
@김젼	나중에 쓰이는거는
@김젼	문제풀때 쓰이는거는
@김젼	어떤문제를 풀지 아는거는 어려워요
@김젼	근데 머신러닝은
@김젼	현장을 갔을때
@김젼	모르는 상황이 많을수록
@김젼	더 위력을 발휘하죠
@김젼	머신러닝은
@김젼	미래를 예측하고싶은거에요
@김젼	알고있는 미래에 대비하는게 아니라
@김젼	관측된 데이터를 가지고
@김젼	에러만 미니마이제이션하면
@김젼	과거데이터에 굉장히 연연하게되어요
@김젼	핵심은 모르는 데이터가 들어왔을떄 하는거지
@김젼	베이지안은
@김젼	출력을 알때
@김젼	입력을 예측할 수 있어
@김젼	굉장히 재밌는 부분이에요
@김젼	내가 저거를 설명한 진짜 이유는
@김젼	이 식을 설명하기위해서였는데
@김젼	쪼인트 프로바빌리티는
@김젼	감독학습은
@김젼	사실은 요고를
@김젼	호호혿
@김젼	이 식을 쓰기위해
@김젼	오늘 수업을 하여따
@김젼	P(Y|X) = P(X,Y)/P(X)
@김젼	좌변은 감독학습이고
@김젼	우변은 무감독학습이죠?
@김젼	수식으로 보면 너무나도 자명해요
@김젼	kcm노트북 계정이름이 unused네
@김젼	둘이 무슨사이에용
@김젼	내가 무감독학습을 해놓으면
@김젼	감독학습에서 알고자하는걸 예측할 수 있어
@p	그렇고 그런 사이
@김젼	1/P(X) 가
@김젼	노말라이즈하는 텀인데
@김젼	자명한거같지만
@김젼	사실 계산 불가능한 term이에요.
@김젼	내가
@김젼	풍경 영상인식을 하겠다고 쳐봐요
@김젼	산을 찍으면
@김젼	토끼, 나무, 자연
@김젼	모든걸 다 인식할 수 있어야돼.
@김젼	모든 사람 얼굴
@김젼	딱 주면
@김젼	이게 무슨오브젝트인지 다 알아맞추고 싶어해요
@김젼	그러면
@김젼	이럴라면
@김젼	모든 우주의 영상에 대한
@김젼	1/P(X) 를 계산할 수 있어야하죠?
@김젼	근데 불가능해요.
@김젼	보지도 않은걸 무슨수로 상상해
@김젼	이부분은
@김젼	이론적으로는 불가능한데
@김젼	프랙티컬하게 1/P(X)를
@김젼	어프록시메이션하는거에요
@김젼	뭔가 영상을 보면
@김젼	이게 치타인거같기도 하고
@김젼	재규어인거같기도 해
@김젼	그러면
@김젼	확률이 더 높은걸로 어프록시메이션하는거에요
@김젼	그래서 여러 머신러닝 모델들이 다 P(X)를 어프록시메이션하는거에요
@김젼	우리가 다루는 모델들은
@김젼	2/3은
@김젼	다 디터미니스틱한 알고리즘들이고
@김젼	다 E 미니마이즈하는거에요
@김젼	현실떄문에 어쩔수가 없는거지
@김젼	근데 연구하는건
@김젼	확률적인 모델들이에요
@김젼	딥러닝 안에서도 역시
@김젼	디터미니스틱한거랑
@김젼	확률적인게 따로 있어
@김젼	예를들어
@김젼	컨벌루젼 뉴럴넷
@김젼	이런건 디터미니스틱한거에요
@김젼	근데
@김젼	디터미니스틱한것이 이런 확률적인 모델의 서브셋이라서
@김젼	제너럴한 이론적인 형태를 먼저 이해하면
@김젼	도움이 될 수 있어
@김젼	다음시간부터는 여기러 오세요?
@김젼	수업긑
```

> 9월 7일 월요일 (보강)

```
@김젼	머신러닝
@김젼	수업 시작
@김젼	장교수님 지각
@김젼	..
@sgk	교수님 지각인데 어떻게 수업을 시작함
@sgk	?
@김젼	옆에 kcm있어서
@김젼	무리는 아님
@sgk	ㅋㅋㅋㅋ
@김젼	옆에서
@김젼	ㅃㅂ형이
@김젼	눈물흘림
@김젼	즙즙
@sgk	헐 그런거 필기노트를 올려야지
@sgk	필기 안 함?
@김젼	삐범
@김젼	요즘
@김젼	IRC 에 중계한다음에
@김젼	나중에 정리하려구요
@sgk	그러니까 그거 IRC 중계를 안하냐고
@sgk	ㅂㅂㅂ이 눈물 흘리는 kcm의 기계학습개론
@sgk	아 그러게
@김젼	즙즙
@sgk	오늘 수업 갔으면 ㅂㅂㅂ을 볼 수 있었는데
@sgk	애가 irc를 끊어서 심리적 거리가 멀어짐
@sgm	김젼 icpc 나가자
@김젼	이닛이닛
@김젼	저 리락쿠마 해드릴수있음
@김젼	ㅇㅅㅇ)
@김젼	ㅇㅅㅇ)
@김젼	ㅇㅅㅇ)
@김젼	ㅇㅅㅇ)b
@김젼	ㅇㅅㅇ)a
@김젼	ㅇㅅㅇ)+
@김젼	ㅇㅅㅇ)c
@sgm	ㅇㅋ
@김젼	매트랩 극혐...
@김젼	드랍하고싶어진다
@sgm	한 명만 더 있으면 되겠군
@김젼	곰돐
@김젼	쩐다 kcm
@김젼	ICPC 가고싶어도 못가
@김젼	캬
@김젼	옵티머스 프라임
@sgm	docs에 너 이름이랑 메일 넣었음
@p	내 이름이랑 메일 넣어
@p	그리고 멸망
@김젼	돜
@sgm	PhoBiA <p@example.com>
@p	아 집에 가야지
@p	ㄶ 뭐해
@sgk	sgm: 박성원이 자기 여친이랑 같이 출전할 사람을 찾고 있던데
@p	ㅋㅋㅋㅋㅋㅋ
@sgk	지현 버리고 거기에 들어가기
@김젼	흑
@p	sgk: 배고파요
@sgk	참아
@p	ㅠㅠ
@p	나쁜인간
@김젼	지금 성원이형 옆에있음
@p	인간도 아냐
@sgk	오오
@sgk	지금 어머니가 수육 만들거 준비하고 계신다
@sgk	사랑합니다
@sgk	나는 수육 먹어야지
@sgk	p: 느그 집에 이런 거 없지
@p	아뇨 있어요
@p	지금없긴한데
@p	있게 만들수 있음
@p	ㅋ
@sgk	그러면 먹으면 되겠네
@p	돈 처바르면 안 되는게 어딨나여
@김젼	octave
@sgm	octave
@sgk	옥타브?
@sgm	matlab
@sgm	open source?
@sgm	version
@sgm	지현 버리고 저기 들어갈까
@김젼	ㅇㅂㅇ
@김젼	._.
@김젼	과제 스펙이
@김젼	매트랩으로 나올뻔하다가
@김젼	학생들의 노력으로
@김젼	matlab C R 파이썬 등 모두
@김젼	허용되는걸로
@김젼	스펙이 바뀌었음
@김젼	캬
@김젼	범준이형: 언어다운 언어를 쓰고싶어요
@즈눅	아희
@김젼	상언: 파이썬 쓰고싶어요
@김젼	나: 매트랩이 공짜가 아니라서
@김젼	와와와와오아ㅗ아
@김젼	C 매트랩 파이썬
@김젼	세개중에 하나로
@김젼	정해졌음
@김젼	사실상 파이썬 쓰라는말이고
@김젼	데이터는 ㅡㅜㅑㄴㅆfksmsd
@김젼	MNIST라는
@김젼	유명한거 쓰고
@김젼	10월 8일 23:59:59 까지.
@김젼	라이브러리는
@김젼	넘파이 사이파이 이런건 써도 되는데
@김젼	theano
@김젼	써도됨
@김젼	C나 파이썬의 경우
@김젼	프롬프트로
@김젼	성능같은걸 띄워주세여
@김젼	.c .cpp .py
@김젼	조교
@김젼	C와 C++ 다른건지 파문
@sgk	ㅋㅋㅋㅋ
@sgk	조교가 수업함?
@sgk	과제설명하고 끝날필인가?
@김젼	C C++ matlab Python
@김젼	네개중 하나 쓰는걸로 변경
@김젼	속도보다
@김젼	애큐러시를 더 중시하겠다
@김젼	보고서와
@김젼	얼마나 잘 구현을 했는지
@김젼	애큐러시도 좀 보고
@김젼	속도는 여기 적혀있듯이
@김젼	한시간이 넘어가면 그냥 안돌아가는걸로
@김젼	이게 기준.
@김젼	실습실에서 돌린다는 가정
@김젼	매틀랩은 설치되어있는데............
@김젼	ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ
@김젼	Q: 실습실에 라이브러리가 안깔려있으면 어떻게 하나요?
@김젼	저도 일단
@김젼	파이썬 세아노 환경을 가지고있으니까
@김젼	Q: GPU 써도 되나요?
@김젼	Q: 학습하는 데이터셋은 몇개쯤 되나요?
@김젼	만개요
@김젼	리드미에
@김젼	실행할떄 쓰는
@김젼	명령어는 명확하게 써서
@김젼	혼동이 없게 해주세요
@김젼	교수님 수업 시작
@김젼	머신러닝은
@김젼	데이타 드리븐이라서
@김젼	데이타 설명을 내가 안했어요
@김젼	80바이 60 이미지가 있으면
@김젼	그냥 글자인식같은경우엔 4800짜리 0, 1 바이너리 픽셀로 생각할 수 있죠?
@김젼	이거 인식결과를
@김젼	1-of-n 인코딩으로 표현해요
@김젼	숫자 2를 표현한다 하면
@김젼	(0, 0, 1, 0, ... )
@김젼	(여기까진 지난시간에 했음)
@김젼	RBF
@김젼	라디안 베이시스 펑션
@김젼	을 쓰자
@김젼	필기인식을 하는것이 바로 우리 플젝.
@김젼	이게 A
@김젼	B는
@김젼	입력은 쇼핑몰 고객 데이터
@sgk	-------------------------------------------- <- 여기까지 읽음
@희종	C와 C++이 다른건지..?
@김젼	조교가 뭘 모르는말을 한거임
@김젼	그 말 하자마자 뒤에서 교수님이
@김젼	C랑 C++은 개념적으로 많이 다르지
@김젼	이랬음
@sgk	역시 장병탁 교수님
@희종	조교님도 컴공 아니시냐
@sgk	수업을 잘 안오시는 거 말고는 훌륭하군
@김젼	성과 나이, 사는곳, 월급 등을
@김젼	모두 1-of-n 인코딩
@김젼	성 {0, 1}
@김젼	나이 {아이, 청소년, 성인, 노인} = {1000, 0100, 0010, 0001}
@김젼	위치 { 동 서 남 북 }
@김젼	월급 { ~5K, 5K~10K, 10K~ }
@김젼	가족사이즈 = {'1', '2', '3', .. '1'}
@김젼	이것들을
@김젼	전부 튜플로 묶어서
@김젼	x 라는 입력으로 나옴
@김젼	다진 데이터가 x = (성, 나이, 위치, 월급, 가족크기)
@김젼	이런식으로 있을때
@김젼	결과 y가
@김젼	{노말, VIP} = {0, 1}
@김젼	이렇게 나옴
@김젼	이런식으로도 입력을 인코딩할 수 있고
@김젼	C. 커피 주문 대화 데이터
@김젼	x = {word1, word2, ... word10000}
@김젼	가 있을떄
@김젼	X = x^10 (단어 열개를 인풋 하나로 봄)
@김젼	출력은
@김젼	y = { 인사, 주문, 질문, 지불 }
@김젼	"커피 한잔 주세요"
@김젼	이게 무슨말인지 알아맞추는 문제임
@김젼	인사인지, 주문인지, 질문인지, 지불인지
@김젼	대화를 네 부류로 나누는 문제임
@김젼	RBF로 학습을 시킨다고 하면
@김젼	Y = { 1000 0100, 0010, 0001 }
@김젼	요렇게 인코딩해요
@김젼	D = {(X, Y)}^N
@김젼	그래서 목표는
@김젼	AI가 커피주문을 대신할 수 있도록
@김젼	만드는것임
@김젼	이건 학생이 한 예시고
@김젼	실제로는 열 아홉가지로 나눴음
@김젼	그 19개는 언어학자들이 정한거임
@김젼	C는
@김젼	성능이 안나오는건데
@김젼	좀 나오게하는거에요
@김젼	A랑 B 는 잘 되는데
@김젼	C는 어려운거라서 프로젝트로 할만해요.
@김젼	이건 이상한 머신러닝의 예에요
@김젼	요거를 다시 내가
@김젼	그
@김젼	지난시간에 아주 뽀말하게만 이야기했는데
@김젼	이걸 다시 추상적으로
@김젼	데이터스페이상에 놓고 얘기할게요
@김젼	머신러닝은
@김젼	데이터스페이스에서 크게 보면 두가지인데
@김젼	지난번에
@김젼	감독 무감독이 있었죠?
@김젼	감독은 보통 분리-분류 해내는 문제에요
@김젼	무감독 학습은 사실은 밀도, 그룹화에요
@김젼	감독학습은
@김젼	무감독학습은
@김젼	데이터가 어디에 모여있는지
@김젼	분류만 하고 말아
@김젼	근데 감독학습은 그걸로 바운더리를 그어요
@김젼	이 바운더리를
@김젼	Decision boundary라고 불러요
@김젼	이 예시는 내가
@김젼	글자를 예로 들었지만
@김젼	글자가 아니어도
@김젼	여러분이 많이 들어본
@김젼	딥러닝같은경우는
@김젼	이런 감독 무감독을
@김젼	같이하는거에요
@김젼	예전에는 이걸 따로했었어
@김젼	밑에서는 무감독학습을 하고 위에서는 감독학습을 해
@김젼	이미지를 예로 들어보면
@김젼	2랑 5를 구분하는거하고
@김젼	2를 마구 모아서
@김젼	2를 생성하는거랑
@김젼	다를수가 있어요
@김젼	구별을 잘 하기 위해선
@김젼	2와 5의 차이점을 학습해야해요
@김젼	감독학습이 이거야
@김젼	근데 무감독 학습은
@김젼	어느어느애들이 어떻게 유사한지에
@김젼	더 관심이 있어요
@김젼	2랑 5의 차이를 배우는거하고
@김젼	2자의 특징이 뭔지, 5의 특징이 뭔지 각각 배우는거랑
@김젼	다를 수 있어요
@김젼	예를들어
@김젼	여러분이 객관식문제를 찍는다고 쳐봐요
@김젼	그냥 답만 맞추면 되지
@김젼	문제의 본질이 뭔지는 알 필요가 없죠?
@김젼	지금 머신러닝이 잘하는데
@김젼	잘하는게
@김젼	분리/분류에요
@김젼	밀도 그룹화 이거는
@김젼	잘한다는걸 설명하기 쉽지않아요.
@김젼	예를들어
@김젼	B를 봐봐요
@김젼	누구누구가 우수고객인지 아는건 쉽죠?
@김젼	이게 감독학습.
@김젼	그냥 데이터 넣고 돌리면 나오겠지
@김젼	근데
@김젼	어떤어떠한 사람이
@김젼	우수고객인지
@김젼	설명하고싶다고 해봐요
@김젼	이 복잡한 데이터에서 경향을 읽어낼 수 있을까?
@김젼	어렵죠
@김젼	머신러닝이
@김젼	통계학하고 다른부분일 수 있는게
@김젼	상당히 엔지니어링해요 머신러닝은
@김젼	데이터를 많이 가지고
@김젼	어쨌든
@김젼	그
@김젼	슥슥
@김젼	데이타분리 해내는
@김젼	이런 경계선을 찾아낼 수 있어
@김젼	ㅇㅅㅇ 몬소리야
@Nemo	왜 경계선이냐면 svm 같은거 하면
@Nemo	경계를 구함
@Nemo	0들이랑 1들이랑
@Nemo	n차원 공간에서 나타내질탠데
@김젼	글쿤
@Nemo	이걸 분리하는 함수를 만들면
@Nemo	저 벡터를 넣었을떄
@Nemo	0 이상 이하
@Nemo	이걸로 판단 가능
@김젼	'
@shasta	 ㅋㅋ
@김젼	감독하고
@김젼	무감독학습의
@김젼	관계를 정의하는게 중요하고
@김젼	이걸 다른식으로 정의하면
@김젼	음
@김젼	아
@김젼	우리가
@김젼	기본적으로 트레이닝 데이타를
@즈눅	whois shasta
@김젼	그니까
@김젼	엑쓰
@김젼	와이
@즈눅	죄송합니다
@김젼	벡타가 내가 쓰기가 힘드니까
@김젼	그냥 이렇게 쓰도록 할꼐요
@김젼	D = {(x, y)}
@김젼	노테이션을
@김젼	그냥 이렇게만 쓸게요
@김젼	갯수 정의 안해도 돼죠?
@김젼	D = {(x, y)}^N
@김젼	그다음에
@김젼	에라함수라는걸 정의할 수 있으요
@김젼	f(x;w) = y'
@김젼	이렇게 나올 수 있고
@김젼	어떤거는
@김젼	f(x;w) = x'
@김젼	위에께 감독학습
@김젼	아래께 무감독학습
@김젼	x'는 x0, x1 이런걸로 이해해주세요
@김젼	(도함수 아님)
@김젼	(?)
@김젼	y = h(x) 감독
@김젼	x = h(x0) 무감독
@김젼	이걸 우리가 일반적으로
@김젼	에라 이런말을 많이 쓸텐데
@김젼	Loss 라는 말을
@김젼	포괄적으로 정의해요
@김젼	손실함수 이런뜻인데
@김젼	Loss : error, distance, difference
@김젼	이게 쫌이따 많이 달라질텐데
@김젼	Loss(x, f(x;w)) = (y - f(x;w))^2
@김젼	내가 로스함수를 이렇게 정의했어
@김젼	이러면 감독학습이에요 이게
@김젼	이게
@김젼	에러 제곱으로
@김젼	로스를 정의한 경우고
@김젼	다른거는
@김젼	Loss(x, f(x;w)) = (x - f(x;w))^2
@김젼	이게 무감독학습이에용
@김젼	차이가 모에요?
@김젼	y가 x로 바뀐거밖에 없죠?
@김젼	모에모에
@김젼	학습한 모델이 생성한 결과하고의 차이
@김젼	외우는건 비슷한거야
@김젼	이런거는 일종의
@김젼	컴프레션같은거에요
@김젼	컴프레션 내지는 컴플리션이라고 그러는데
@김젼	완성하는 문제
@김젼	그리고 또 하나
@김젼	Risk 라는걸 정의할거에요
@김젼	누가
@김젼	인테그랄 기호좀 (?)
@shasta	∫
@김젼	감사합니다
@김젼	+_
@김젼	R(w) = ∫x,y Loss(x, f(x;w))P(x,y)dxdy
@김젼	의미는
@김젼	모든 x, y 에 대해..
@김젼	 Loss(x, f(x;w))P(x,y)dxdy를 더함
@김젼	그니까 그냥
@김젼	가능할수있는 모든 경우의 로스를
@김젼	확률밀도에 곱해서 다 더하는거군
@김젼	Risk
@김젼	계산하는게
@김젼	불가능하겠죠?
@김젼	리스크의 정의는
@김젼	로스의 기대치에요.
@김젼	이세상에 존재하는
@김젼	모오든 데이터
@김젼	정의역에 있는 모오든 데이터에 대한
@김젼	로스의 기대치를 계산한것이 바로
@김젼	리스크에요
@김젼	미니마이즈하고싶은것이 바로
@김젼	이 R(w)를 미니마이즈하고싶은거에요
@김젼	근데 여기서
@김젼	로스를
@김젼	다양한 기준으로 정의할 수 있어요
@김젼	아 그러쿠나
@김젼	Loss(x, f(x;w))가 아니라
@김젼	Loss(y, f(x;w))에용
@김젼	몬소린가했네
@김젼	ㅋㅋ
@김젼	Loss(y, f(x;w)) = (y - f(x;w))^2 감독
@김젼	무감독은 vise versa
@김젼	Loss를
@김젼	에러의 제곱뿐만 아니라
@김젼	다양한 기준으으로
@김젼	Q: 저 리스크에서 적분할때에
@김젼	넹
@김젼	저 에이가 뭔가 ㅁ느능르
@김젼	잘 안들림
@김젼	Q:끄러면은 아까 처음에
@김젼	어그 데이터 내에서
@김젼	필기체 문제 인식하는걸 예로 들면
@김젼	교수님: 학습이 뭐냐면은
@김젼	학습이 일반화라고 그랬잖아
@김젼	여기 점이있는거만 학습을 했는데
@김젼	실제로 알고싶은건
@김젼	내가 테스트데이터로 주지 않은 값들이 어떤 결과를 낼지도 알고싶은거잖아?
@김젼	그래서
@김젼	아까 필기인식을 예로 들면
@Nemo	정의가 안되는 데이터를 넣을 필욘 없어요
@Nemo	이건 내가 궁금하던거였ㄴㄴ데
@김젼	즈엉의
@김젼	이걸
@김젼	적분이 아니라
@김젼	덧셈으로 표시해도 돼
@Nemo	시구마
@김젼	엠퍼리컬 리스크라고 그러는데 이걸 (뭔지 잘 못들음)
@shasta	교수님이 처음 쓰신 R(w)는 우리가 영원히 알 수 없는 값
@김젼	R(w) = Sigma_x Sigma_y Loss(y, f(x;w))P(x,y)
@김젼	오옷...
@즈눅	Empirical risk minimization
@김젼	1년 가까이 잠입해 계시던 샤스타님을 머신러닝 이야기로 꺠웠다 (?)
@김젼	ㅇㅅㅇ)+
@김젼	ㅌㅌㅌ (?)
@김젼	[영어] - 'Empirical' 검색결과 -> empirical // [형용사] (주로 명사 앞에 씀) 경험[실험]에 의거한, 실증적인
@김젼	d에 사실
@김젼	감독 무감독은
@김젼	어떻게 보면
@김젼	교과서나 이런데에선
@김젼	이렇게 하진 않아요
@김젼	이건 내가 오래 생각한건데
@김젼	감독 무감독의 경계가
@김젼	명확하진 않아요
@김젼	그래서 내가 이걸
@김젼	처음으로 시도하는건데
@김젼	모든걸 무감독학습식으로
@김젼	내가
@김젼	확률으로 보면
@김젼	좀더 명확하다고 했죠?
@김젼	감독학습은 P(y|X)
@김젼	무감독학습은 P(X, y)
@김젼	그래서 무감독학습을 하면
@shasta	이상한.. 교수님 정의..
@김젼	감독학습도 할 수 있다고 했으요
@Nemo	저렇게 생각하니 좀 신박하다
@김젼	P(y|X) = P(X,y)/P(X)
@Nemo	이상한건가
@김젼	근데 거꾸로는 안되고
@Nemo	ㅋㅋㅋ
@김젼	혹시 다른 질문?
@김젼	여기 상세한건 별로 없으요
@김젼	아주 추상적인거야
@김젼	f가 어떤모양인지
@김젼	P가 어떤모양인지
@김젼	이건 얘기 안했지
@김젼	그리고 R(w)를 어떻게 추정하는지
@김젼	이런것도 얘기 안했쬬?
@김젼	이게 다 머신러닝이 다뤄야 하는 문제인데
@shasta	교수님 의도는 알겠는데.. 교과서랑 너무 다름
@김젼	우린 아주 큰 그림에서 머신러닝이 뭘 얘기하는지를
@김젼	이야기하는거고
@김젼	원래로 돌아가보면
@김젼	아 인제
@김젼	머신러닝이
@김젼	크게보면
@김젼	사실은
@김젼	기계학습이라는거를
@김젼	모뇨노로
@김젼	(잘안들림)
@김젼	기계학습이나 머신러닝이나
@김젼	같은말입니다
@김젼	어어
@김젼	함수로 굳이 한다면은
@김젼	L : D -> M
@김젼	이거하는 문제에요
@김젼	데이터는 누차 나왔지만
@김젼	굳이 써보면은
@김젼	D 는 { X x Y }^N 의 부분집합
@김젼	M 은 S x W 의 부분집합
@sgm	졸리다
@Nemo	머신러닝은 데이터로부터 모델 만들기
@김젼	learn이라는거는
@김젼	데이터로부터 모델 만들기
@김젼	데이터가 주어졌을떄
@김젼	모델만드는문제인데
@김젼	모델은
@김젼	내가 간단하게 썼을때엔
@김젼	w 라고 썼고
@김젼	더간단하게 쓰면 m이라고 쓰면
@김젼	쓸텐데
@김젼	사실 두개가 있어요
@김젼	S 구조와 x W 파라미터
@김젼	스트럭쳐와 웨이트에요
@김젼	그래서 사실은
@김젼	머신러니잉
@김젼	이거이거 찾는문제에요
@김젼	L : D -> M
@김젼	간단하쬬?
@Nemo	참 쉽죠?
@김젼	근데 엄청난 스페이스에요
@김젼	잘생각해보면
@김젼	스페이쓰의 크기가
@김젼	어마어마해요
@김젼	우우쥬!
@김젼	예를들어서
@김젼	어
@김젼	그
@김젼	교수님 말이 느려서
@김젼	거의 토씨 그대로 다적는중
@Nemo	ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ
@김젼	그래서 지금 해야되는게 뭐냐면은
@김젼	우리가 뭘할거나면
@김젼	이게 어떤 모양이냐
@김젼	S 가 어떤모양이냐 가
@김젼	첫번째고
@김젼	그다음에는
@김젼	아까 얘기한거처럼
@김젼	이 로스를 정의하는게
@김젼	상당히 중요한거고
@김젼	오브젝티브 뻥션이에요
@김젼	이 제곱 말고도
@김젼	여러가지 모양으로 변형할 수 있고
@김젼	로스라고 안부르기도 해
@김젼	그다음에는
@김젼	L 학습하는 알고리즘이 필요해
@김젼	데이터가지고 알고리즘 만드는게 필요해
@김젼	S
@김젼	Loss
@김젼	Learn
@김젼	이 세가지가
@김젼	주 고민할거리에요
@김젼	오늘은 이 세가지에 대해 맛보기만 할거에요
@Nemo	radial basis function
@김젼	그리고 앞으로 여러분들이 할거는
@김젼	RBF로
@김젼	네트워크를 만들어서
@김젼	학습을 하고
@김젼	어떻게 트레이닝을 하는가느
@김젼	데이타로부터
@Nemo	은 결국 euclidean distance 가지고 하는거 같은데
@김젼	에라를 미니마이즈하도록
@김젼	과정을 뭔가 해야지
@김젼	학습이 되는거야
@김젼	세가지가 액션이에요
@김젼	정의를 다시해보면
@김젼	하나는
@김젼	어
@김젼	여기다쓸까
@김젼	1
@김젼	1. 모델의 구조
@김젼	2. 목적함수. 학습의 objective function
@김젼	3. 학습 알고리즘
@김젼	이 세개가
@sgm	이거 로그 긁어서 커밋함?
@김젼	머신러닝의 세가지 목적이에요
@김젼	당장은 로그 긁어서 커밋하고
@김젼	나중에 정리할거에요
@김젼	모델의 구조를 어떤거 쓸거냐
@김젼	이게 연구하는사람이에요
@김젼	딥러닝!
@김젼	층이 마않은 모델구조를 써요
@김젼	이번시간에
@김젼	구조 몇개를 알아볼게요
@김젼	그 전에
@김젼	모델 구조를 살펴볼텐데
@김젼	아 참고로 필기중에 여기다가 말하시면
@김젼	제가 그대로 깃헙에 올리니
@김젼	주의
@김젼	종류가 아무리 많아도
@김젼	잘 생각해봤더니
@김젼	사실은 학습을 하는 방법이
@김젼	기본적으로는
@김젼	아이엠언 어썸가이
@김젼	모델의
@김젼	### 모델을 구성하는 요소
@김젼	를 내가 고민을 해봤더니
@김젼	세가지밖에 없는거같애
@김젼	1. instances
@김젼	2. rules
@김젼	3. functions
@김젼	이 세가지밖에 없어요
@김젼	잘 생각해봤더니
@김젼	모델을 어떻게 만들거냐!
@김젼	가장 쉬운방법은
@김젼	데이타를 통쨰로 저장하고, 조합을 해서 예측하는거에요
@김젼	그게 1
@김젼	그 다음에
@김젼	2는 예~전에 하던 AI가 하던거에요
@김젼	전문가알고리즘처럼
@Nemo	http://www.mbmlbook.com/toc.html Model based Machine Learning
@김젼	if elif를 마구 엮어서
@김젼	강남에 사는
@김젼	중년 부부
@김젼	쀼
@김젼	강남에 산다..
@김젼	나이가 중년이다..
@김젼	쀼다..
@김젼	1번 2번은
@김젼	학습이 잘 안돼요
@김젼	대부분의 머신러닝은
@김젼	3번 이거를 쓰고있어요
@김젼	function
@김젼	근데 뻥션이 뭐냐
@김젼	다 그냥 방정식이에요
@김젼	이퀘이션으로 많이 해
@김젼	대부분은 뻥션이에요
@김젼	모델의 종류는
@김젼	이게 수업에서 하겠지만
@김젼	한가지는
@김젼	1. 뉴럴네트워크같은 구조에요
@김젼	거미쥴
@김젼	나이가 있었고
@김젼	성별이 있었고
@김젼	이게 우수고객이냐 아니냐
@김젼	우수고객이면 1 아니면 0
@김젼	(대충 뉴럴넷 구조 설명하셨음)
@김젼	벡터가 들어가서
@김젼	퍄퍄 하고 나옴
@김젼	f(x;w) = w1x1 + w2x2 + w3x3 + ...
@김젼	선형결합!
@김젼	실제로는 좀더 다른게 있는데
@김젼	그냥 대략적으로는 이래요
@김젼	결국은 함수로 나와요
@김젼	선형함수
@김젼	여러 변수들의 선형결합으로 나와요
@Nemo	실제론 w0x0, x0=
@김젼	이게 뉴럴 네트워크의 기본이에요
@Nemo	1
@김젼	뀪?
@Nemo	을 씀니당
@김젼	ㅇㅅㅇ?
@김젼	그게 몬소리임
@Nemo	저기에
@Nemo	상수 텀
@sgm	가상의 feature를 더해서
@Nemo	을 넣어줌
@sgm	1로 놓음
@김젼	ㅇㅎ
@김젼	왜해?
@Nemo	w0*x0로 하고 x0를 보통 1로고정
@김젼	이게 perceptron이에요
@김젼	왜하는거야?
@Nemo	왜냐면 상수가 있어야 좀더 정확하게 할수 있어서?
@김젼	perception하는 기계
@김젼	또 다른거는
@김젼	디씨젼 트리
@김젼	ㅇㅎ
@김젼	상수라는말이군
@Nemo	ㅇㅇ
@김젼	성별이 뭐냐
@김젼	남자냐
@김젼	여자냐
@김젼	남잔데
@김젼	아까 뭐가있죠
@김젼	사는지역이 어디냐
@김젼	(대충 디시젼 트리 구조 설명하시는중)
@김젼	(디시젼 트리 브랜치가 너무 많아져서 고생중)
@김젼	이래서 데이터가 정수인게 있을때에 트리가 별로 안좋아요
@김젼	하여간
@김젼	입력이 쭈르르 내려가서
@김젼	뭐이면 VIP
@김젼	뭐이면 노말
@김젼	데이타를 계속 쪼개요
@김젼	데이타를 원하는 특징이 나올때까지 쪼개요
@김젼	여기는 로스함수가 재밌는데
@김젼	엔트로피라는 개념을 써요
@김젼	엔트로피 들어봐쬬?
@김젼	물리/화학시간이요
@김젼	정보이론에서도 엔트로피를 써요
@김젼	화학에서의 개념을 가져와서 쓰는거긴 한데
@김젼	엔트로피는 불확실성의 측정이에요
@김젼	데이터의 엔트로피를 잴 수있어요
@김젼	어떤게 엔트로피가 높은 데이타냐면은
@김젼	고객 데이터를 만개를 가져왔는데
@김젼	그중에 우수고객이 5000
@김젼	노말이 5000이라고 쳐봐요
@김젼	이게 엔트로피가 높아요 낮아요?
@김젼	반대로
@김젼	데이타를 10000개를 가져왔는데
@김젼	우수고객객이 10명이야
@김젼	이게 A
@김젼	우수고객이 5000명인게 B
@김젼	어느게 더 맞추기 쉽겠어요?
@김젼	나: A요
@김젼	왜죠?
@김젼	그냥 다 노말이라고 치면 되니까요
@김젼	그러쵸
@김젼	거저먹고 들어가는거죠
@김젼	A가 더 엔트로피가 낮죠
@김젼	B는 맞추기 어려운문제에요.
@김젼	B는 엔트로피가 더 높아요
@김젼	머신러닝으로 학습시키기 더 어려운 데이터에요.
@김젼	디시젼 트리는
@김젼	데이터를 자꾸 분류해서
@김젼	엔트로피를 낮추는 과정이에요.
@김젼	예를들어
@김젼	이 쇼핑몰
@김젼	남자 VIP가 거의 없다
@김젼	그럼 남성으로 잘라요
@김젼	또는
@김젼	샐러리 10k이하에는 VIP가 더더욱 없다
@김젼	이러면 샐러리로 먼저 짜르면
@김젼	엔트로피를 더 줄일수있겠죠?
@김젼	이래서
@김젼	디시젼 트리에선 로스를 엔트로피로 정의해요
@김젼	지금은 그냥 엄밀하게 정의 안하고 설명만 할게용?
@김젼	여러분이 프로젝트를 해야되는
@김젼	RBF는
@김젼	뉴럴넷하고 거의 똑같애요
@김젼	근데 저거보다
@김젼	복잡하다고 봐야되는데
@김젼	여기 층이 하나 더 있는
@김젼	ㅇㅅㅇ
@김젼	어
@김젼	다시그리며는
@김젼	RBF는
@김젼	여기가 x1 x2 x3 x4인데
@김젼	여기가 히든 RBF y라고 보면은
@김젼	*h라고 보면은
@김젼	h1 h2 h3
@김젼	중간노드가 있어
@김젼	그리고 이걸 다시 결합해서
@김젼	디시젼 바운더리를 만들어
@김젼	요 만드는 모양이
@김젼	가우시안 함수에요
@김젼	이런모양을 가진.
@김젼	입력스페이스에서
@김젼	이 쎈타가 있고
@김젼	거리(시그마)가 있어가지고
@김젼	이 거리가 원형일수도 있고, 타원일수도있고 몰라요
@김젼	여튼 이런걸 여러개 모아가지고
@김젼	데이타를 분포를 추정해요
@김젼	마지막 층은
@김젼	디시전 바운더리를 여기다가 만들어내
@김젼	이렇게 보면
@김젼	뉴럴넷같죠?
@김젼	한층을 더하는거야
@김젼	이런게 전형적인 멀티레이어 퍼쎕트론
@김젼	뉴럴넷인데
@김젼	이거 말고는
@김젼	어
@김젼	사진을 떠 찍어
@김젼	어뜨케
@김젼	촬영타임
@김젼	여기
@김젼	1. NN
@김젼	2. 디씨젼 트리
@김젼	3. RBF
@김젼	얘기했어요 지금
@김젼	4. 는
@김젼	4. SOM
@김젼	쎌프 오거나이징 맵
@김젼	이거는 입력이 이렇게 있는데
@김젼	출력을 2차원 평면상에다가 매핑해요
@김젼	격자모양으로
@김젼	지도를 그려요
@김젼	그리고 이게
@김젼	다 연결되어있어
@김젼	촷촷
@김젼	무슨모양인지
@김젼	알게쬬?
@김젼	다연결된거야
@김젼	x1, x2, x3, x4 가 입력이면
@김젼	출력이
@김젼	y11, y12, y13, ..., y44
@김젼	입력이
@김젼	3을 줬을때엔
@김젼	얘네들이 ON 되고
@김젼	4가 들어왔을때엔 얘네들이 ON되고
@김젼	이게
@김젼	생물학적으로 실제 뇌의 구조랑
@김젼	닮았다고 해요.
@김젼	topology-preserving map
@김젼	이라고 부르는데
@김젼	물리적인 세계에서 인접해있으면
@김젼	그림으로 그린 세계에서도
@김젼	가까이 있어요.
@김젼	이 2랑 이 2랑
@김젼	비슷하게 생겼다면
@김젼	SOM에 비췄을떄
@김젼	활성화되는 모양도
@김젼	비슷할거다
@김젼	이런소리에요
@김젼	특징은 지도를 그려줘요
@김젼	토폴로지를 유지하는 맵을 그려줘요
@김젼	그래서 아주 유용한 툴인데
@김젼	아주 복잡한, 이상한
@김젼	사람이 알아보기 힘든 패턴이 들어와도
@김젼	출력에선
@김젼	가까이 그림을 그려주기땜 ㄴㅇㄹㄴㅇㄹ
@김젼	가까이 그림을 그려주기때문에
@김젼	이게 덴시티나 클러스터나 등등 하는데에 유리해요
@김젼	지금까지는
@김젼	내가 입력을
@김젼	항상 1렬로 유지했어
@김젼	그리고 그들간의 관계가 없었어요
@김젼	근데 그게아니고
@김젼	5.
@김젼	입력간의 관계를
@김젼	그림으로 그릴 수 있어
@김젼	일반화 시키면
@김젼	그래프 구조가 되어요
@김젼	이놈이 이거하고 연결되고
@김젼	이거는 이거하고 연결되고
@김젼	이건 이거하고 연결되고
@김젼	쫜쫜
@김젼	(x1 ~ xn 점들이 유향그래프로 연결됨)
@김젼	아까 입력 변수들인데
@김젼	그래프 구조를 가져요
@김젼	이걸
@김젼	베이지안 네트워크라고 그래
@김젼	5. 베이지안 네트워크
@김젼	DAG
@김젼	BN
@김젼	DAG가 뭐죠?
@김젼	다이렉티드 에이 사이클릭 그래프
@김젼	방향성은 있으나, 사이클이 없는 그래프.
@김젼	베이지안 네트워크라고 그래요
@김젼	왜 베이지안이냐
@김젼	베이스 룰을 써서
@김젼	추론을 해요
@김젼	앞에있는걸로 뒤에꺼를 영향주고 그래요
@김젼	이게 보통 쓰이는 예는
@김젼	크레딧카드 도용 감지.
@김젼	이게 머신러닝이나 데이터마이닝이
@김젼	일찍 적용된 예시에요.
@김젼	시부엉 우리나라는 왜 이런거 없엉..
@김젼	ㅇㅅㅇ (?)
@김젼	결재회사가
@김젼	이 사람이
@김젼	causality, 인과관계를 어느정도 나타내요 이 네트워크가
@김젼	이 사람이
@김젼	기름을 채웠다고 쳐봐요
@김젼	그러면 그 다음 결제는
@김젼	상대적으로 차를 타고 멀리 가서 다른곳에서 할
@김젼	확률이 늘어나죠?
@김젼	데이터가지고 그래프 만드는문제에요
@김젼	이것도 주로 무감독학습에 써요
@김젼	덴스 에스티메이션에 주로 쓰니까.
@김젼	P(x1, x2, .. , xn) 이걸 어떻게 표시할건지의 문제에요
@김젼	베이지안 네트워크 이런게 있고
@김젼	6.
@김젼	고다음에
@김젼	어
@김젼	고다음에 더 복잡한거
@김젼	베이지안 네트워크 '같은거'인데
@김젼	하이퍼넷
@김젼	내지는 하이퍼그래프 구조인데
@김젼	아까 여기서는
@김젼	에지가
@김젼	노드당 두개만 있었죠?
@김젼	내가 엣지를
@김젼	노드를 세개가진 엣지를 정의할 수 있어
@김젼	이게 하이퍼 그래프에요
@김젼	네개엣지 가진 노드도 정의할 수 있어
@김젼	문제는 이게
@김젼	그림으로 표시가 안돼요 이게
@김젼	아
@김젼	뭔지알겠다
@김젼	*네개의 노드를 가진 엣지를 정의할 수 있어요
@김젼	이렇게
@김젼	하이퍼엣지를 만들어서
@김젼	이 H들 안에있는 노드들끼리는
@김젼	서로 연결된.
@김젼	H!
@김젼	임의에 더 큰거의 조각으 ..
@김젼	내가 또 이렇게 조각을 낼 수 있어
@김젼	결국은 이것도 마찬가지에요
@김젼	덴시티 에스티메이션인데
@김젼	원래 데이터를 표현하는 또다른 방법이에요.
@김젼	장점은 뭐냐면
@김젼	복잡한 관계를 한꺼번에 표시해요
@김젼	그래서
@김젼	데이터를 표현하는 또다른 방법이고?
@김젼	내가 살펴볼거고
@김젼	그리고 지금 많이 하는
@김젼	7. DNN
@김젼	딥뉴럴네트워크라는거는
@김젼	뉴럴넷인데?
@김젼	층이많어.
@김젼	여기도 층이있고
@김젼	층이있고
@김젼	층이있고
@김젼	요런 층을
@김젼	촥촥촥
@김젼	층을 그에속 쌓아가요
@김젼	층이 많아서
@김젼	딥다크 하다고 해서 딥 뉴럴네트워크에요
@김젼	디시젼 바운더리가
@김젼	아주복잡해도
@김젼	만들어낼 수 있어요
@김젼	물론
@김젼	시간이 많이걸리고
@김젼	학습하는데에 복잡도는 더 커요
@김젼	그다음에는 우리가 사실은
@김젼	수업시간에 우리가 여기는 안다룰거같은데
@김젼	대학원에서 다루는건데
@김젼	8.
@김젼	HMM
@김젼	히든 마르코프 모델.
@김젼	이거는 어떤 구조냐면은
@김젼	체인구조에요.
@김젼	그래프구조죠?
@김젼	일반적으로는 그래프에요
@김젼	마르코프 체인이라고 하는데
@김젼	그리고 이런것들은 변수를 나타내
@김젼	->ㅇ->ㅇ->ㅇ->ㅇ->ㅇ
@김젼	  |   |   |   |   |
@김젼	  ㅇ  ㅇ  ㅇ  ㅇ  ㅇ
@김젼	지금 여러분들이 가진
@김젼	대부분의 데이터는
@김젼	안녕하세요 라고 하면
@김젼	이걸 끝까지 받은다음에
@김젼	한번에 영상처럼 처리하는데,
@김젼	HMM은
@김젼	시간이 흐름에 따라
@김젼	스르륵 학습을 하려는거에요
@김젼	잘 안돼.
@김젼	시간에 대한 체인구조를 가졌다는거고
@김젼	9. RNN
@김젼	Recurrent Neural Network
@김젼	지금까지는
@김젼	뉴럴넷이 층이 여러개 있어도
@김젼	앞으로만 진행했는데
@김젼	NN, RBF 다 그런데
@김젼	RNN은
@김젼	사이클을 허용해요
@김젼	이놈이 이렇게 들어가고 (뒤에서 앞으로 거꾸로 감)
@김젼	재귀구조를 가져쬬?
@김젼	이게 아직 연구대상이에요
@김젼	출력 나온놈이 입력에 영향을 또미쳐
@김젼	시간이 감에 따라
@김젼	근데 이게 사실은
@김젼	아주
@김젼	좋은 모델이에요
@김젼	언어처리라든지.
@김젼	우리가 하는걸 생각해보면
@김젼	내가 예측한게
@김젼	그 다음시간에 들어오는 데이터를
@김젼	영향을 줄 수 있어요
@김젼	그래서 리커런트 네트워크는
@김젼	재귀적 구죠를 가져쓰요
@김젼	DNN은
@김젼	딮이어도
@김젼	재귀구조는 안가져요
@김젼	이걸
@김젼	feed-forward
@김젼	9번 RNN이걸
@김젼	recurrent라고 하고
@김젼	그..
@sgm	지친다
@김젼	우리가 다룰거는
@김젼	한
@김젼	7번까지만 다룰거같애
@김젼	나머지는 대학원가면
@김젼	근데 굳이 하나 더 해야한다면은
@김젼	10.
@김젼	예를들면
@김젼	여러분 들어봤을텐데
@김젼	CN
@김젼	Complex Network
@김젼	머신러닝 관점에선 아직 안다루고있는데
@김젼	복잡계 막의 특성이
@김젼	몇가지 알려져있어요
@김젼	예를들면
@김젼	브레인 네트워크가
@김젼	복잡계 망인데
@김젼	이런식이에요
@김젼	이거 고딩때 들었던거같다
@김젼	어떤곳에 군집되어있고
@김젼	이런걸 허브라고 그래
@김젼	허브끼리는 가끔 연결되어있고
@김젼	허브가 아닌애들끼리는 멀리 듬성듬성 연결되어있고
@김젼	일부는 군집화되어있고
@김젼	일부는 안되어있어요
@김젼	랜덤네트워크와 레귤러 네트워크의 중간이에요
@김젼	모여있으면 장점이
@김젼	이놈들간에 통신을 빨리 쉽게 안정적으로 할 수 있겠죠?
@김젼	원거리 통신하려면 허브를 거쳐가면 돼요
@김젼	그..
@김젼	젤
@김젼	인터넷 구조가
@김젼	저런식이에요
@김젼	서울대에 들어오면
@김젼	클래스 B 두개가 빽빽하게 있는데
@김젼	저기 논산가면 한산하고
@김젼	대학교같은데가 허브가 되고
@김젼	아니면 공항망
@김젼	항공사마다
@김젼	자기네 항로가 쫙 거미줄처럼 표시되어있어요
@김젼	주로 어디로 가는지
@김젼	코드쉐어
@김젼	갑자기 항덕수업이 됨
@김젼	뇌가
@김젼	이런 구조를 쓰는걸로
@김젼	알려져있어요
@김젼	유전자네트워크 프로틴네트워크도 비슷해요
@김젼	자 결국
@김젼	학습이라는게
@김젼	데이터를 표현하는
@김젼	또다른 방법인데
@김젼	이게 전부 데이타 변수라고 그러면
@김젼	자주나타나는건
@김젼	압축해서 표현하고
@김젼	잘 안나타나는거는 추가로 표현하면 되게쬬?
@김젼	구우조가 아주 중요해요
@김젼	쫌..
@김젼	쉬었다해야되게쬬?
@김젼	많이갔는데
@김젼	되게많이갔는데
@김젼	월래는 내가
@김젼	두시간만 할라그랬는데
@김젼	진도를 너무 못나갔는데?
@김젼	먼얘기를 하다 이렇게됐지?
@김젼	쪼끔 그래도 진도 나가야할거같은데?
@김젼	쉬는시간도중
@김젼	DNN으로
@김젼	무감독학습 어떻게 하는지,
@김젼	무감독학습 해놓고 그걸로 분리분류
@김젼	어떻게 하는지
@김젼	질문했고
@김젼	대충 추상적인 대답만 얻음
@shasta	stacked autoencoder
@김젼	오옷
@김젼	오오옹옷
@shasta	가지고 계속 이야기하시는듯 ㅋ
@김젼	ㅇㅂㅇ...!
@김젼	http://ufldl.stanford.edu/wiki/images/thumb/5/5c/Stacked_Combined.png/500px-Stacked_Combined.png
@김젼	우왕..
@김젼	머신러닝 모델이
@김젼	왜이렇게 많아요?
@김젼	라는 질문은
@김젼	프로그래밍언어가
@김젼	왜이렇게 많아요?
@김젼	라는 질문이랑
@김젼	비슷해요
@김젼	DNN.
@김젼	파워풀해요
@김젼	이걸로 모든 문제를 커버할 수 있을지도 몰라요
@김젼	근데 너무 헤비해요
@김젼	전기를 엄청나게 잡아먹고
@김젼	시간을 다쓰고
@김젼	답은 천천히 나오고
@김젼	그럼 못쓰겠죠?
@김젼	아주 작은 문제를 풀때엔
@김젼	다른 작은걸 써도 되잖아요
@김젼	이런걸 설명하는 이론이 있어요
@김젼	중요해요
@김젼	여러분들도 알아두세요
@김젼	여러분 인생에도 도움이 될거에요
@김젼	No Free Lunch Tehorem
@김젼	No Free Lunch Theorem
@김젼	이걸
@김젼	웹페이지에
@김젼	수십페이지로
@김젼	증명한사람이 있어요.
@김젼	내가
@김젼	슬기형이
@김젼	점심을 맛있는걸 사줘서
@김젼	가도
@김젼	내가 대신에
@김젼	정보를 주거나
@김젼	뭔가 주는게 있어요
@김젼	심지어 아주 중요한 정보를 전달하고 줄지도 몰라
@김젼	내가 무슨 머신러닝 알고리즘이나 모델을 써도
@김젼	각 모델이 잘 푸는 문제와
@김젼	잘 못푸는 문제가 있어요
@김젼	풀려고하는 문제는 잘풀어.
@김젼	근데 똑같은 문제를 다른문제에다가 적용하면
@김젼	못풀죠?
@김젼	여기에 특화되어있기때문에.
@김젼	No Free Lunch Theorem이 아주 추상적인 이야기인데
@김젼	알고리즘 A1 A2 가 있어요
@김젼	그리고 문제가
@김젼	P1, P2, ... 가 있어요
@김젼	A1가지고 각 문제들을 모두 풀고 성능을 평가하고
@김젼	A2가지고 각 문제들을 모두 풀고 성능을 평가하고
@김젼	그리고 그 성능의 평균치를 내면
@김젼	같다
@김젼	에요.
@김젼	N3N에서 본 O(n^3) 버블소트도 과연 (?)
@김젼	보고보고소트 (?)
@김젼	여러분이
@김젼	알아야하는건
@김젼	내가 뭔가를 잘하려고 하면
@김젼	놓치는게 있다는 사실을
@김젼	인정해야해요
@sgk	뭑
@sgk	내가 점심을 사면 김젼이 중요한 정보를 준다고?
@김젼	ㅇㅇ
@sgk	주식정보라도 주나?
@김젼	맨날 주잖음
@sgk	돈되는 정보를 내놔라
@sgk	지금 당장 현금화 할수 있는 정보
@sgk	로또번호같은거
@김젼	엔화를 파세요
@sgk	내가 1주일은 봐준다
@김젼	2 3 5 7 9 11
@김젼	마법의 김젼고둥
@김젼	불확정성원리같은거에요
@김젼	위치를 제대로 재려고하면
@김젼	운동량 리졸루션이 떨어지고 그러지
@김젼	여러분이
@김젼	프로그램을 나중에 짜도
@김젼	애큐러시하고 속도하고
@김젼	트레이브오프 관계에요
@김젼	회사가서 코딩을한다
@김젼	학교에서 하는거처럼
@김젼	문제잘풀고 이런것만 중요하지 않아요
@김젼	메모리 작은데에다가 집어넣어야 한다던가
@김젼	전기를 조금써야한다던가
@김젼	그럼 트레이드오프 관계가 있는거
@김젼	알긴 알아야하기때문에
@김젼	재밌죠?
@김젼	이름이.
@김젼	너무 욕심내면 안돼
@김젼	둘다 욕심내는건 불가능하다는
@김젼	철학적인 의미가 이쓰요
@김젼	첫번째는 모델 구조를 했고
@김젼	이제 여러가지 오브젝티브 뻥션들을 알아봅시다.
@김젼	이게 이제..
@김젼	얘기하면
@김젼	한학기가 지나는 주제에요
@김젼	이게 중요하다그랬죠
@김젼	학습의 목적이 뭐냐
@김젼	학습을 어디로 drive 할거냐.
@김젼	아까 예로 든거
@김젼	- 스퀘어드 에러.
@김젼	  E = sigma (y - f)^2
@김젼	그다음에
@김젼	- classification accuracy
@김젼	f의 결과, y가 discrete한 값이면
@김젼	클래시피케이션 정확도를 측정할 수 있어용
@김젼	맞게 분류한거랑 틀리게 분류한것들 갯수를 셀 수 있어용
@김젼	나중에
@김젼	SVM같은거 할때 쓰는말인데
@김젼	- margin
@김젼	그리고 확률로 가면
@김젼	- Likelihood를 정의하는경우가 이써
@김젼	라이클리후드는
@김젼	지난번에 베이지안할떄 썼는데에에
@김젼	베이지안으으을 정확히 설명을 안해쓰요
@김젼	ANG
@김젼	원래는
@김젼	이걸로합시다.
@김젼	P(w|x) = P(x|w)P(w)/P(x)
@김젼	베이지안으로 학습할때에는
@김젼	그롸롸.....
@김젼	x가 데이터
@김젼	w가 우리가 관측하고싶은 현상
@김젼	아 지친다
@김젼	이떄 여기에서
@김젼	P(w|x)를 postrior
@김젼	P(x|w)를 likelihood
@김젼	포스테리어는 사후확률 사건이 생긴 후 확률
@김젼	P(w)는 prior 사전확률. 사건이 생길 확률
@김젼	P(x)는 evidence, 계산하기 어렵다고 여러번 이야기했죠
@김젼	그래서 실제계산에선 이건 무시해
@김젼	likelihood는 무슨뜻이느냐
@김젼	P(w). 데이터가 없어요
@김젼	추론해야돼
@김젼	P(x|w)는 데이터를 관측해
@김젼	모델로부터 데이터가 생성될 확률
@김젼	1, 2, 3, 4 숫자로부터 이 AI가 '1', '2', '3', '4' 그림이 생겨날 확률
@김젼	데이타를 안보고
@김젼	추측을 해보고
@김젼	업데이트를 해서
@김젼	새로운 모델을 만들고
@김젼	이걸 계속 반복해
@김젼	베이지안 inference가
@김젼	데이터로부터 모델을 계속 학습시키는 방법이에요
@김젼	이게몬소리여
@김젼	likelihood를 로스로 보겠다는게 무슨뜻이느냐
@김젼	P(x|w)만 맥시마이즈하겠다는거에요
@김젼	근데 그러지않겠다는게 무엇이냐
@김젼	- Bayesian
@김젼	P(w|x) = P(x|w)P(w)/P(x) 전체를 다 향상시키겠다
@김젼	그리고 중간이 있어
@김젼	어..
@김젼	예를들면
@김젼	- Penalized Likelihood
@김젼	likelihood를 그냥 하는게 아니고
@김젼	P(x|w)P(w) 를 둘다 고려해서
@김젼	최대화하겠다.
@김젼	사실은 실제로는
@김젼	이 확률들 아에다가
@김젼	로그를 취해서 써요.
@김젼	확률을 그대로 취하지 않고.
@김젼	확률값이 낮기때문에..
@김젼	그래서 사실은
@김젼	- Log likelihood를 써
@김젼	  log P(x|w)
@김젼	그다음에 아까 얘기한
@김젼	리스크도 있고
@김젼	- rist
@김젼	- cost, utility, value 이런걸 쓰기도 해요
@김젼	머신러닝이나 AI가 약간 경제학적인 용어를 쓰기도 해.
@김젼	가치함수, 효용함수
@김젼	장기같은거 둘떄 쓰는거에요
@김젼	장기자랑..
@김젼	쮸왓쮸왓..
@김젼	반대가 되면
@김젼	코스트가 되죠?
@김젼	돈이 제일 많이들거나
@김젼	에너지를 제일 많이 쓰거나
@김젼	힘이 드는
@김젼	이런것들이 다 이게
@김젼	변형들인데
@김젼	어... 이거를
@김젼	리스크는
@김젼	지워버려쓰요
@김젼	지워버려써
@김젼	예를들어
@김젼	로그에 대해서
@김젼	로스함수를
@김젼	어..
@김젼	기대치를
@김젼	로스를
@김젼	접근흚..........
@김젼	어...
@김젼	요기까진 해야되는데?
@김젼	그 .........
@김젼	학습 알고리즘을
@김젼	나중에 할때 하겠는데
@김젼	식이 있으니까 먼저 해보면
@김젼	베이지안같은경우
@김젼	이게
@김젼	사실은
@김젼	분포에요 분포.
@김젼	P(w|x)
@김젼	이게 분포에요.
@김젼	분포를 추정하는거는 어려운일이에요.
@김젼	그래서 베이지안이 계산을 못하는 어려운거였는데
@김젼	옛날에 베이지안 안쓴다그랬죠?
@김젼	컴퓨팅 파워가 좋아져서 이제는 해요.
@김젼	그래서 어떻게 하냐면은
@김젼	실제로는 베이지안을 계산하는게 아니고
@김젼	이거를
@김젼	- MAP이라그러는데
@김젼	맥시멈 어프~ ㅍ~
@김젼	이거의 max를 구해
@김젼	이 분포가 아니라 최대지점만 봐요
@김젼	맥스만 계산해요
@김젼	요 분포를 맥스로 하는 지점만 계산해서
@김젼	이 모델 하나를 가지고
@김젼	P(w|x)를
@김젼	대체해버려.
@김젼	이 W* 로
@김젼	전체를 대표시켜
@김젼	이게 실제로 엔지니어들이 practical하게 잘하는거에요.
@김젼	이렇게 산이 크게 하나로 만들어진경우엔
@김젼	잘 하겠죠?
@김젼	근데 산이 꼭대기가 두개인 모양이면
@김젼	좀 어렵죠.
@김젼	진짜 베이지안은
@김젼	최대지점만 고르는게 아니라
@김젼	다 계산하는건데
@김젼	시간이 오래걸려요..
@김젼	그래서 이걸
@김젼	IBM 왓슨
@김젼	들어봤어요?
@김젼	얘는
@김젼	앙상블 머신이에요.
@김젼	그냥
@김젼	있는 AI 모델을
@김젼	다써봐요.
@김젼	있는 AI를 다만들어서
@김젼	다 조합해서
@김젼	쓰는게 앙상블머신.
@김젼	베이지안을 생각해보면 이게 좋은방법이에요.
@김젼	무조건 다쓴다고
@김젼	좋지 않을 수 있어요.
@김젼	이모델 저모델 다써서 평균냈다가
@김젼	베스트가 희석되는 경우가 있어요.
@김젼	이건
@김젼	우리가
@김젼	어려운 의사결정을
@김젼	10명 커미티가 모여서
@김젼	회의를 한다음
@김젼	보팅하거나, 평균내는건데
@김젼	이것도 단점이 있다는걸 알아야돼요
@김젼	지금 회사에서 의사결정
@김젼	이렇게 안하잖아요
@김젼	저렇게 여럿이 모여서 의논해서 결과내는건
@김젼	통계적으로 안정된 의사결정을 하려고하는건데
@김젼	대신 시간이 오래걸리죠
@김젼	여러분이 회의 해서 알지만.
@김젼	산으로 가잖아요
@김젼	의사결정에 중요한 expert가
@김젼	발언건을 얻기 쉽지 않을수도 있어
@김젼	이거를 다 안다고 생각하고
@김젼	한가지만 정의를 할게요.
@김젼	엔트로피이 가
@김젼	재밌는
@김젼	정의인데
@김젼	우와
@김젼	나 청각장애인
@김젼	속기사 해도 되겠다
@김젼	정신놓고 그냥 다적고있었네
@김젼	엔트로피는
@김젼	H로 쓰는데
@김젼	H = - Sigma_(k=1~m) Pk log Pk
@김젼	http://patentimages.storage.googleapis.com/WO2014046404A2/PCTKR2013008019-appb-I000002.png
@김젼	이 식 쓰셨음
@김젼	P0 = 0.7
@김젼	P1 = 0.3
@김젼	하나는 이렇고
@김젼	하나는
@김젼	P0 = 0.6
@김젼	P1 = 0.4
@김젼	위에께 더 엔트로피가 낮죠?
@김젼	이게 어떤차이가 있는지
@김젼	정확히 메져하는게
@김젼	엔트로피에요.
@김젼	어떻게 계산하겠느냐 의 문제인데.
@김젼	여기다가 식을 집어넣어보면
@김젼	저 둘이 어떻게 다른지 바로 알 수 있어요
@김젼	log (1/Pk)
@김젼	이건 확률이 높을수록 작아지는 숫자죠?
@김젼	그리고 Pk를 곱한건
@김젼	확률밀도를 곱한거에요.
@김젼	오홍
@김젼	그렇구나
@김젼	ㅇㅅㅇ)b
@김젼	log(1/Pk)
@김젼	이걸 information이라고 정의해요
@김젼	정보.
@김젼	여기서 정보라는거는 어떤 의미냐면
@김젼	9시뉴스는 지났는데
@김젼	10뉴스를 가서 들어
@김젼	그럼 별로 좋은뉴스가 아니죠?
@김젼	새로운 얘기를 해야 뉴스지.
@김젼	surprise 를 이야기하자
@김젼	놀라운거
@김젼	평소에 못접하던 새로운 소식
@김젼	평소에 못접하던거여야 그게 정보지.
@김젼	그래서
@김젼	확률이 작은데 값이 높도록
@김젼	정의했어요
@김젼	그래서
@김젼	엔트로피 H는
@김젼	평균 정보량이에요.
@김젼	오호
@김젼	IRC 채널별 엔트로피 계산하기
@김젼	여기선
@김젼	불확실한게 정보가 많다고 보는거에요
@김젼	내가 다 아는데에서 사건을 해가지고 얻어내봤자
@김젼	새로운 지식습득이 없죠?
@김젼	나중에 지식습득할떄
@김젼	Informaion gain이라면서 이 용어가 다시 나오는데
@김젼	내가 맨날
@김젼	아는 지역에 가봤자
@김젼	정보가 없잖아
@김젼	모르는 지역을 가야지
@김젼	모험을 해야지
@김젼	어드벤쳐!
@김젼	ㅃㅂ: 와 멋지다
@김젼	그래서 이거의 변형들이 많이 쓰이는데
@김젼	사실 딥러닝도 이 메져를 씁니다
@김젼	어답티브 엔트로피
@김젼	가 아니구나
@김젼	Relative entropi
@김젼	아니 어걸 렐라티브라고 발음하다니
@김젼	너무해
@김젼	이걸
@김젼	KL 다이버전스라고도 이야기하는데
@김젼	KL(P || Q) = Sigma P(x) log P(x)/Q(x)
@김젼	분포의 거리에요.
@김젼	P라는 분호랑 Q라는 분포의 디스턴스에요
@김젼	아니지
@김젼	디스턴스가 아니고
@김젼	다이버전스에요
@김젼	이게
@김젼	P || Q, Q||P
@김젼	뒤집으면 달라서
@김젼	거리가 아니에요.
@김젼	이게
@김젼	풀어서 써봐요
@김젼	KL(P || Q) = Sigma P(x) log P(x) - Sigma P(x) log Q(x)
@김젼	        = - H(P) + H(P, Q)
@김젼	여기서
@김젼	H(P, Q)가
@김젼	크로스 엔트로피에요
@김젼	Q의 엔트로피를
@김젼	P의 분포로
@김젼	측정하는거에요
@김젼	KL(P || Q) = H(P, Q) - H(P)
@김젼	실제로는
@김젼	릴레이티브 엔트로피가
@김젼	많은곳에 퍼져있고
@김젼	콘트래시브 다이버전스라고도 하고
@김젼	크로스 엔트로피까지 내가 얘기했어
@김젼	크로스 엔트로피도 종종 쓰이는 개념
@김젼	머신러닝 관점에서 보면
@김젼	이게 어떤 의미를 가지냐면
@김젼	P는 내가 알고있는 세상의 데이터의 분포에요
@김젼	Q는 내가 학습을 하고있는 모델이에요
@김젼	학습한 모델
@김젼	내가 만들고있는 모델
@김젼	학습의 목표가 뭐야
@김젼	원래있는 데이터를 잘 재현하는거죠?
@김젼	계속 이미테이션하는거야
@김젼	내가 인공지능을 만들잖아?
@김젼	보통 사람을 흉내내려고 해
@김젼	그 데이터를 모아서
@김젼	내 행동 고대로 하려고 하는거야
@김젼	P가 이상적인 선생님
@김젼	Q가 학습자.
@김젼	이게 얼마나 학습을 잘하는가는
@김젼	둘의 거리가 0이 되면 되겠지?
@김젼	그래서
@김젼	이 거리를
@김젼	미니마이즈 하면 돼
@김젼	문자인식
@김젼	트레이닝 패턴이
@김젼	정해져있는데
@김젼	그 데이터를 가지고
@김젼	원래를 흉내내면 되요
@김젼	제너럴하게는 무감독학습 개념 이죠?
@김젼	어
@김젼	하나만 더 해야되는데
@김젼	그다음 쓰는게
@김젼	뮤추얼 인뽀메이션이에요
@김젼	그러면 끝인데
@김젼	MI
@김젼	뮤추얼 인뽀메이션이 뭐냐면
@김젼	뮤추얼 인뽀메이션이 뭐냐면
@김젼	X; Y
@김젼	두개
@김젼	인스탄시에이트된거하고 다시 (잘안들림)
@김젼	MI(X; Y) = KL( P(x,y) || P(x)P(y) )
@김젼	이거에요.
@김젼	이게 정의에요
@김젼	P대신 조인트 프로바빌리티를 넣고
@김젼	Q대신 마지날 프로바빌리티를 넣었죠?
@김젼	이걸 그대로 단순 수식 계산해보면
@김젼	MI(X; Y) = Sigma Sigma P(x, y) log( P(x,y)/P(x)P(y) ) for all x in X, y in Y
@김젼	이게
@김젼	P의 조인트 프로바빌리티는
@김젼	P(x, y) = P(x)P(y|x) = P(y)P(x|y) 이렇게 배우고
@김젼	흔히 x랑 y가 독립이면
@김젼	P(y|x) = P(y)라고 하죠?
@김젼	이게
@김젼	조인트 프로바빌리티랑
@김젼	그냥 곱한거랑
@김젼	거리를 봐요
@김젼	만약에 두개가 독립이면
@김젼	MI(X;Y) = 0
@김젼	만약에
@김젼	x랑 y랑 완전 종속이면
@김젼	나중에
@김젼	https://en.wikipedia.org/wiki/Mutual_information
@김젼	오늘의 수업 요약
@김젼	앞으로 한학기동안 배울걸
@김젼	대충 한번 쓱 훑은거에요
@김젼	알고리즘은
@김젼	아직 안다뤘어요
@김젼	요번주 화 목이 수업이 없고
@김젼	알아서 하고싶은걸 하시고
@김젼	엔트로피에
@김젼	로그씌우는 이유
@김젼	내가 원하는 정보를
@김젼	표현하기위한
@김젼	비트의 수
@김젼	I(X; X) = H(X)
```

> 9월 15일

```
@Nemo	9/17 10/1 휴강 9/22 7:00-10:00 로 대체
@김젼	- 모델 구조
@김젼	- 목적함수
@김젼	- 학습 알고리즘
@김젼	데이타로부터 모델 만드는것.
@김젼	L : D -> M
@Nemo	Error
@Nemo	Classification rate
@Nemo	Loss
@Nemo	Risk
@Nemo	Entropy
@Nemo	Relative Entropy
@Nemo	Mutual info
@김젼	엔트로피가
@김젼	불확실성을 나타내는 척도에요.
@Nemo	확률에 반비례하게
@Nemo	log_2 를 취하면 비트수
@김젼	인포메이션 씨오리
@김젼	컴공엔 없고
@김젼	전기과 대학원에 나중에 있는데
@Nemo	유투브에 강의 있음
@김젼	코딩 이런거
@김젼	정보량!
@김젼	(지난시간에 했던것들 계속 복습중)
@Nemo	오늘밤 9시 뉴스에 들은 얘기 계속 나오면 정보량이 낮아
@Nemo	놀라운 얘기는 확률이 낮은거임
@Nemo	확률이 낮은게 정보량이 많아
@sgk	 역시 지난시간 보강 안 온게 이득
@김젼	Fallback divergence
@Nemo	렐레티브 엔트로피랑 크로스 엔트로피랑 같은거군
@김젼	슬슬 복습 끝남
@Nemo	폴백 다이버젼스라니
@Nemo	Kullback–Leibler divergence
@김젼	저게
@김젼	뮤추얼 인포메이션이던가?
@Nemo	아 크로스엔트로피는 다른거구나
@Nemo	KL(P,Q) = H(P,Q) - H(P)
@김젼	H(P, Q) <- 크로스 엔트로피
@김젼	KL 다이버전스
@김젼	KL(P || Q)
@김젼	P와 Q의 거리
@김젼	라고 지난번에 이야기했던듯
@Nemo	KL(P||Q) != KL(Q||P) 라 distance 비스무리한거
@김젼	아 맞아
@김젼	디스턴스가 아니라
@김젼	다이버전스
@김젼	ㄱㅅㄱㅅ
@Nemo	앵
@김젼	뮤추얼 인포메이션
@김젼	MI(X; Y) = KL( P(x,y) || P(x)P(y) )
@Nemo	값이 크면 서로 dependent
@Nemo	MI(X;Y) = MI(Y;X)
@Nemo	대칭
@김젼	Y를 예측하는데에
@김젼	어떤 X 를 쓰늑넛이
@김젼	좋을지
@p	캐치볼하고 싶당
@Nemo	나 꼬시면 할지도(?)
@p	ㄷㄷ
@p	그렇군
@Unused	캐치유 캐치유 캐치미 캐치미~
@p	하지않으면 공으로 때리겠다!
@Nemo	ㄷㄷ
@Unused	이제 숨바꼭질은 그만
@Unused	(그만)
@Unused	우울한 건! 모두!
@Unused	파란 하늘에!!!
@김젼	목적함수
@김젼	실제적으로는
@김젼	에러를 많이 써요!
@p	개념을 파란 하늘에 날렸냐
@Unused	너?
@Nemo	세상의 많은 것들은 가우시안
@김젼	수능을
@p	너
@김젼	아무리 어렵게 내든
@김젼	쉽게 내든
@Unused	너
@김젼	가우시안이 나와요.
@Nemo	N이 충분히 크면
@김젼	언유도
@p	그래 언유즈드놈
@김젼	가우시안입니다.
@김젼	회사에서 야근하는 포비아도
@Unused	포비아새기
@김젼	가우시안입니다.
@김젼	수업시간에 IRC하는 김젼도
@김젼	가우시안입니다.
@Unused	뭐
@Unused	포비아가 가우시안이라고
@김젼	가우시안의 마음을 담아서.
@김젼	가우시안스 룰
@김젼	가우시안.
@Unused	적분해버려야지
@Nemo	statistical physics
@Nemo	p(x) = 1/z * exp(-(E(x))/K_BT)
@김젼	K는 볼쯔만 상수
@김젼	E는 에러, 에너지
@김젼	P는 확률
@Unused	에러
@p	실책은 안 좋지
@Nemo	Probaboloty (posterior, likelihood)
@Nemo	앗 오타
@Nemo	ㅠㅠ
@Nemo	parameter(theta)에 대한 likelihood
@Nemo	log P(x|theta) 는 -E(x|theta) 에 비례
@Nemo	그러므로 error 줄이기나 likelihood 늘리기나 같은거
@김젼	장교수님
@김젼	지난 보강때
@김젼	어디까지 했는데
@김젼	기억이 안나시나
@김젼	아니야
@김젼	복습하면 좋지뭐
@김젼	P(sigma|D) = P(D|sigma)P(sigma) / P(D)
@김젼	P(sigma|D) posterior
@김젼	P(D|sigma) likelihood
@김젼	theta_map = argmax P(theta|D)
@김젼	MAP
@김젼	맥시멈 어프록시메이션
@김젼	뭐시기
@김젼	80% 이상은
@김젼	ML이라는걸 하는데
@김젼	이거는 뭐냐면
@김젼	theta_ml = argmax P(D|theta)
@김젼	likelihood 만 계산하는거에여
@sgk	정말이지 설명 잘 하시는 거 같운데
@sgk	뭔 말인지 모르겠다
@김젼	저게
@sgk	아니 뭔 말인지는 알겠는데
@김젼	지금까지 수업하면서
@김젼	했던 수업
@김젼	을
@김젼	데자뷰처럼
@김젼	반복하면서
@sgk	어떻게 쓴다는건지 감이 안 잡힘
@김젼	사이사이에
@김젼	새로운 내용이있는거라
@김젼	이거
@김젼	실제로 쓴는거랑은
@김젼	거의 상관없는
@김젼	수업일듯?
@김젼	적어도 지금은.
@sgk	나 분명히 어제 로그에서
@sgk	김젼이 수업시간에 잔 적 없다는 말을 본거 같은데
@sgk	이제 그 말 하면 까야지
@Nemo	ㅋㅋㅋㅋㅋㅋㅋ
@Nemo	꾸벅꾸벅
@sgk	맨 앞자리에서
@Nemo	헤드뱅잉
@sgk	이제는 자리깔고 자는군
@김젼	너무피곤하다
@김젼	P(D) = integral P(D|theta)P(theta)dtheta
@김젼	가우시안 분포다 하면
@김젼	계산할 수 있어요
@김젼	그게 아니다면 추론해야돼요
@김젼	여기에
@김젼	P를 썼지만
@김젼	P를 어떻게 표현하는지는
@김젼	머신러닝 모델마다
@김젼	P를 표현하는법이
@김젼	완전히 다 달라요.
@김젼	그 예를 하나 볼텐데
@김젼	조고의 마지막이 될텐데
@김젼	그래서 조끔 지연이 되었는데
@sgm	조금 지현이 됐는데
@김젼	와 이거
@김젼	독학 어떻게하지?
@김젼	학습률을 한번
@김젼	유도를 해보는데
@김젼	얘가
@sgm	example을 먼저 하고
@sgm	들어야
@김젼	목적함수를
@sgm	이해되는 수업 같은데
@김젼	Log likelihood
@김젼	로 정하겠어요
@김젼	그롸롸..
@김젼	log likelihood를 맥시마이즈 하는걸 학습이라고보겠다
@김젼	데이터 D
@김젼	L(w) = log P(D|W)
@김젼	이걸
@김젼	w로 편미분하면
@김젼	dL(w)/dw 가 0으로 수렴하는
@김젼	걸 찾는게 기본 테크닉
@김젼	학습을
@김젼	더더욱 해도
@김젼	L 이 변하지 않으며
@김젼	않으면
@김젼	학습이 끝났다는뜻
@Nemo	gradient descent
@Nemo	라고 부름
@김젼	우왕..
@Nemo	이런 함수들은 convex하다는 가정 하에 쓸수있어
@김젼	ㅇㅎㅇㅎ
@Nemo	그래야 수렴하겠지
@김젼	가우시안 분포
@Nemo	그리고 극점이 여러곳 있으면 한군데로 빠짐
@Nemo	local minima
@김젼	x ~ N(x, 뮤, sigma^2)
@Nemo	neural net같은거에 저런 일이 많이 일어나는데
@Nemo	사실 상관이 없는게
@Nemo	그정도 성능으로도 충분해서
@Nemo	물론 이거 해결하려고 여러가지 기법들이 있긴 함
@김젼	가우시안 분포의 정의에 따라
@김젼	P(x|w) = (상수) exp(- (x-뮤)^2/2sigma^2 )
@김젼	우리의 경우
@Nemo	여기 보면 미분할때 모든 데이터에 대해서 log likelihood를 구해서 gradient를 계산하는데 실제로 저렇게 학습시키면 시간도 많이들고 local minima에 들기 때문에 한번에 데이터 조금만 아님 심지어 1개씩만 써서 학슶키는걸 stochastic gd 라고 부름
@Nemo	전자는 사실 batch 라고 부르고
@김젼	크앙!
@김젼	도댗
@김젼	도대체
@김젼	이거 왜하는거야!
@김젼	시부엉!
@김젼	부엉부엉
@sgk	부먹부먹
@김젼	볶먹볶먹
@Nemo	결국은 optimization problem이야
@김젼	기야아아ㅏㅏㅏㅏㅏㅏㅏㅏ
@Nemo	ㅋㅋㅋㅋㅋㅋ
@Nemo	아 배곺
@김젼	아놔
@김젼	나처럼 펜 없는사람은
@김젼	필기는 어떻게 하라고
@김젼	수식을 저렇게 많이쓰나!
@김젼	포기
@김젼	보나마나
@김젼	저거 편미분하면
@김젼	x ~ 뮤 인데에서
@Nemo	LaTex를 배웁시다
@Nemo	ㅠㅠ
@김젼	0 나오겠지 뭐
@김젼	ㅋ
@Nemo	나도 배워야되는데
@disjukr_talk	휴대폰으로 찍으면 안되나여=3
@Nemo	실제로 조교님이 칠판 사진 찍어서
@Nemo	수업끝나고 올려주십니다
@김젼	헐
@김젼	진짜?
@Nemo	ㅇㅇ bi.snu.ac.kr 가서
@disjukr_talk	두둥
@Nemo	courses
@Nemo	에 machine learning 들어가면 있어
@김젼	헐....
@김젼	http://bi.snu.ac.kr/Courses/ML2015f/ML2015.html
@김젼	착한분이셔써..
@김젼	저거
@김젼	당연히
@김젼	뮤가 x일때에
@김젼	0으로 가는게 아니냐는..
@김젼	뮤 = 1/N sigma x(d)
@Nemo	당연한건데
@Nemo	이제 가우시안이 아니라 일반적인 경우에도
@Nemo	같은 방식으로
@Nemo	ㅇㅇ
@김젼	그렇구나
@Nemo	그 1차원 linear regression 같은 거면 당연히 수직선 위의 점들의 평균
@Nemo	2차원이 되면 weight가 2개고 n차원이면 n개고 뉴럴넷 같은건 무지 많고
@김젼	ㅠㅠ
@Nemo	그래도 원리는 같어
@Nemo	사실 linear regression은 수학적으로도 풀수 있는데
@Nemo	least mean square problem
@Nemo	normal equation으로 해결가능
@김젼	batch learning
@김젼	데이터를
@김젼	한번에 싹다 가르치기
@Nemo	통째로
@김젼	뮤ㅜ우웅
@Nemo	아까 말한 stochastic과 대비되는 개념
@Nemo	이거의 중간으로 minibatch
@김젼	stochastic은
@김젼	데이터
@김젼	점진적으로
@김젼	늘려나가고
@Nemo	1개씩
@김젼	뮤도 변하는
@김젼	미니배치는 그냥
@김젼	N개씩 늘리는?
@Nemo	ㅇㅇㅇ
@김젼	수업끝
@김젼	ㄱㅅㄱㅅㄱㅅ
```

> 9월 22일

```
@김젼	기계학습이라는걸 우리가 어떻게 정의했냐면
@김젼	데이타로부터 모델을 만드는거에요
@김젼	모델은 파라미터를 가지고있다
@김젼	L : D -> M(w)
@김젼	웨이트
@김젼	뉴럴넷에서 출발했기때문에
@김젼	어떤 러닝하는 함수는
@김젼	데이터를 가지고있으면
@김젼	약간 인뽀말하게
@김젼	엉
@김젼	데이탄느 보통
@김젼	감독학습이면
@김젼	(복습중)
@김젼	머신러닝에선
@김젼	오브젝트뻥션이 중요해요
@김젼	장기게임이면 장기룰을 가르쳐줘야해고
@김젼	주식투자면 수익률을 알려줘야하죠
@김젼	고부분이 머신러닝에서 이론적인 부분이에요
@김젼	우선은
@김젼	아직 개요니까.
@김젼	여러번 나왔지만
@김젼	보통은
@김젼	에라함수를 써요
@김젼	theta map
@김젼	theta에 대해 미분해서
@김젼	학습을 더 시켜도
@김젼	P(theta|x)가 변하지 않는 지점을 찾아요
@김젼	어떤거의 확률분포는
@김젼	P(x) = 1/Z exp(- E(x)/(k_b T)
@김젼	열역학에서 쓰는 정의
@김젼	자연계에 존재하는 분포이고
@김젼	머신러닝에서도 이걸 가져다 쓴다
@김젼	k_b T = 1 로 놔버리고
@김젼	P(x) = k exp(- E(x) )
@김젼	E는 에너지
@김젼	로그취하면
@김젼	log P(x) ~= -E(x)
@김젼	max log P(x) ~= min E(x)
@김젼	\theta_map = argmax P(theta|x) = argmax P(x|theta) P(theta) = argmin{E1(x|theta) + E2(theta)}
@김젼	E1 = Error, accuracy
@김젼	E2 = complexivity, regularization
@김젼	E2
@김젼	가능하면 간단한 모델을 선호함
@김젼	오컴의 면도날
@김젼	통계학적인 관점에서 오컴의 면도날의 의미는 ]
@김젼	불확정성을 배제함에 있으
@김젼	배리언스를 줄여요
@김젼	내가 문제를 내고
@김젼	어떤사람은 100라인으로 문제를 풀고
@김젼	어떤사람은 10000라인으로 풀었어요
@김젼	그리고 둘다 답이 잘 나와
@김젼	그러면 100라인짜리를 선호하라 이거죠
@sgm	100라인 보다 1라인을 선호하라
@sgm	망함
@김젼	[1,4,231,43,44,352,].sort
@김젼	E = KL(P || Q)
@김젼	다이버전스
@김젼	P와 Q라는 두 분포의
@김젼	다이버전스, 거리에요
@김젼	이걸 마치 에러라고 생각하기
@sgm	반복 여러번 해주시니까
@김젼	이거 딥러닝에서 쓰이는 모델이에요
@김젼	반복 여러번 해주시니 좋긴하네요
@sgm	ㅇㅇ
@김젼	혼자 따로 공부 안했는데
@김젼	슬슬 그림이 그려지는듯
@sgk	쩐다
@김젼	MI(X; Y) = KL(P(X,Y)||P(X)P(Y)) \theta_ML
@김젼	이게
@김젼	큰 복습이었어요
@김젼	질문
@김젼	세타가 뭔가요
@김젼	뉴럴넷에선 웨이트,
@Nemo	좀 늘어진다는 감이 있는데.
@Nemo	ㅠㅠ
@김젼	가우시안 분포에선 뮤와 시그마?
@김젼	잘하는사람만
@Nemo	그럴땐 장병탁 코치님 짤을 한번씩 https://www.google.co.kr/search?q=%EC%96%91%EC%83%81%EB%AC%B8&newwindow=1&es_sm=122&source=lnms&tbm=isch&sa=X&ved=0CAcQ_AUoAWoVChMIorDPlseJyAIVQyCmCh1KbgbU
@김젼	듣는 수업이 아니라서
@김젼	이런식이 더 나은거같음
@Nemo	머 개론이긴 하니까
@김젼	설명을 무슨
@김젼	예술적으로 잘 해서
@김젼	한번듣고 이해할수있으면
@Nemo	근데 플젝이 10/3인데 리그레션도 안하고 뉴럴넷부터 할거같아...
@김젼	더 좋겠지만
@김젼	그러지 않으신걸
@김젼	스스로 아시니까
@김젼	4주쨰 계속 반복하시는거겠지
@김젼	그리고 닌 다 알잖아
@unused	초고수
@Nemo	머 딱히 그런것도 아니지만
@김젼	츤데레같은놈
@김젼	ㄸ... 딱히 비숍책을 다 읽어온건 아니라굿 *ㅇㅅㅇ*
@김젼	>ㅅㅇ
@김젼봇	ㅇㅅ<
@김젼	ㅇㅅㅇ)b
@김젼봇	d(ㅇㅅㅇ
@김젼	데이타로부터
@김젼	파라미터를 찾는게
@김젼	학습이에요
@김젼	여기는
@김젼	세타 찾는과정을 내가 설명을 안했지만
@김젼	에라를 이렇게 정의하면
@김젼	이걸 미니마이즈하는 세타를 찾을 수 있어요
@Nemo	헐 비숍 펼쳐보지도 않았는데
@김젼	그게 학습알고리즘이에요
@김젼	컴퓨터로 봤나보네 (?)
@김젼	그럼 펼치지 않지
@김젼	ㅎ~
@김젼	(?)
@Nemo	ㄷㄷ
@김젼	가우시안에서
@김젼	수학적으로 min 찾을 수 있었죠?
@김젼	가우시안이 아닌곳에서는
@김젼	알고리즘으로 찾는거양
@김젼	"진짜" 예로 한번
@김젼	들어가보겠습니다
@김젼	(수업 분위기 반전)
@김젼	신경망
@Nemo	오마이갓 진짜 뉴럴넷 하는건가
@김젼	Neural Network
@김젼	ANN
@김젼	Multilayer Perceptron
@김젼	꺄아아악
@Nemo	뭐 배웠다고 벌써...
@김젼	요            정
@김젼
@김젼	등              장
@김젼	이거의 가장 간단한 케이스로
@김젼	i) Perceptron
@김젼	ii) MLP (멀티레이어 퍼셉트론)
@김젼	보통 신경망하면 MLP에요
@Nemo	우리 리그레션 안했자나여 ㅠㅠㅠ
@김젼	Perceptron하면 어떤거냐면
@김젼	입력이
@김젼	x1
@김젼	x2
@김젼	...
@김젼	zn
@김젼	이렇게 있으면
@김젼	얘네들이 모여서
@김젼	-------> ㅇ ------- yn
@김젼	자
@김젼	(x, y), w = (w1, w2, ...., wn)이 있죠
@김젼	데이터, model
@김젼	이죠?
@김젼	에러는 어떻게 정의하느냐
@김젼	E = y - f(x;w)
@김젼	E = (y - f(x;w))^2
@김젼	이게
@김젼	샘플 퍼셉트론이에요
@김젼	멀티레이어 퍼셉트론은
@김젼	똑같은데
@김젼	중간에
@Nemo	보통 xTw를 받아서 f(xTw)로 y를 예측함
@김젼	이런놈들이 여러개있어
@김젼	x1 x2 x3 x4 x5 ........ xn
@김젼	              |
@김젼	   | | |    |  | | |
@김젼	                |
@김젼	y1 y2 y3 y4 ................ yn
@김젼	알아서 이해하십시오
@김젼	중간에
@김젼	히든 뉴런이 이쬬?
@김젼	커넥트는 모든 노드가 모든 노드하고 다 되어있어요
@김젼	다아 연결되어이쓰요
@김젼	Fully connected
@김젼	이게 MLP에요
@김젼	이걸 뭐라그러냐면
@김젼	Fully-connected feed-forward network
@김젼	이렇게도 불러요
@김젼	이게
@김젼	히든이 있어서
@김젼	샘플 퍼셉트론이랑
@김젼	달라지는거야
@김젼	질문: 상수항은 어디갔어요?
@김젼	예습을 많이 했나본데
@김젼	투명노드가 있고 상수항이 있어요
@김젼	Bias term이 필요한데
@김젼	정확한건 나중에 다시할께
@김젼	FUlly connected feed forward network고
@김젼	iii) DNN
@김젼	Deep Neural Network
@김젼	그림으로 그리면
@Nemo	레이어가 디따 많아여
@김젼	(시계방향으로 90도 회전)
@김젼	최근에 구글이 한건
@김젼	22층이었어요
@김젼	지난 20년동안 이거(MLP)만 써왔는데
@김젼	갑자기 스카이 스크래퍼를 만들기 시작했어요
@김젼	===============================
@김젼	                |
@김젼	===============================
@김젼	                   |
@김젼	============================
@김젼	                   |
@sgm	얍
@김젼	                 ......
@김젼	                        |
@김젼	=================================
@김젼	                     |
@김젼	====================================
@김젼	엄청난 컴퓨팅 파워를 필요로 해요.
@김젼	이 구체적인 모양은
@김젼	다를수있어
@김젼	컨벌루전 네트워크일수도있고 뭐일수도있고
@김젼	이번학기에
@김젼	잘하면
@김젼	Perceptron이랑 MLP
@김젼	두개를
@Nemo	아는 사람은 이해할 수 있는 내용들을 설명하신다
@김젼	잘하면 끝낼 수 있어요.
@김젼	난 잘 모르는게
@김젼	난 잘 모르는데
@김젼	대충 뭔소린진 알겠어
@김젼	컴퓨터
@김젼	유연성이에요 유연ㅅ어
@김젼	성
@김젼	사람은
@김젼	되게 유연해
@김젼	여러분 입사시험볼떄
@김젼	면접을 보잖아
@김젼	그러면으
@김젼	이상한 질문 일부러 해요
@김젼	사회성이나 개인 성향도 보고
@김젼	지식만 묻는게 아냐
@김젼	AI에서
@김젼	회사 입사시험 통과하는 로봇 만드는게
@Nemo	튜어링 테스트
@김젼	예전부터 꿈이었어요
@김젼	공부할떄
@김젼	여러분ㅇ
@김젼	내가 많을걸 이야기하잖아?
@김젼	그 유연성이
@김젼	어디서 오냐면
@김젼	줌인 줌아웃이에요
@김젼	보통 수업을 하면
@김젼	액씨엄부터 얘기해서
@김젼	씨오림 만들고
@김젼	결론 얘기하고
@김젼	내가 수업할때엔
@김젼	이렇게
@김젼	엄청나게 전체를 얘기했다가
@김젼	엄청나게 확대를 했다가
@김젼	여러분이
@김젼	공부할떄
@김젼	이렇게
@김젼	크게 작게를
@김젼	피보팅을 잘해야
@김젼	써먹을 수 있어요
@김젼	MLP만 완전히 공부하고
@김젼	퍼셉트론만 완전히 공부하면
@김젼	각각의 대답은 잘 해요
@김젼	근데
@김젼	조각조각만 알지말고
@김젼	세개가 어떻게 다르냐?
@김젼	배운걸 써먹을라면
@김젼	이렇게 숲과 나무를
@김젼	둘다볼수있어야해요
@김젼	이게 굉장히 어려워요
@김젼	그래서 내가 연습하려고
@김젼	수업을 이렇게 하는거에요
@김젼	와 감탄
@김젼	시험문제가
@김젼	이런식으로
@김젼	나오니
@김젼	유의하세요
@김젼	(찰칵0
@김젼	얼굴은 찍지마
@김젼	지금 하는게
@김젼	Perceptron의 학습규칙을 유도할거에요
@김젼	그리고 MLP는 i)로 학습규칙을 유도할거고
@김젼	iii)도 엔지니어링을 할거에요
@김젼	근데사실
@김젼	학계 안에서 볼떄엔
@김젼	별로 브레잌쓰루가 아니에요
@김젼	이론이 막 달라졌냐
@김젼	그렇지 않아요
@김젼	근데 밖에서 볼때엔 브레잌쓰루지
@김젼	지금i) 에서 ii)로 넘어가는데
@김젼	중간에 레이어가 들어갔죠?
@Nemo	내가 얼마전에 컴퓨팅 파워 새로운 컴퓨팅 패러다임 이런 얘기를 누구랑 한거 같은데 누구였지
@김젼	이
@김젼	가운데에 숨어있는
@김젼	히든 뉴런들을
@김젼	밖에서 관측할수없어요
@김젼	히든변수
@김젼	레이턴트 배리어블이에요
@김젼	1958년부터 1986년이니까
@김젼	28년걸렸어요
@김젼	퍼셉트론에서 MLP로 오는데에
@김젼	28년
@Nemo	왜냐면 DNN은 결국 컴퓨팅 파워의 발전 + 무지막지한 데이터가 이뤄낸 거라고 말할수 있는데
@김젼	근데
@김젼	MLP에서 DNN으로 가는데에 또
@김젼	26년이 걸렸어요
@김젼	오 비슷하네
@김젼	Perceptron에서 DNN으로 가는데에
@김젼	반세기가 필요했죠?
@김젼	이 간단한 문제에서
@김젼	이 복잡한 모델로 갔고
@김젼	풀수있는 문제의 범위도
@김젼	엄청 많아졌어요
@김젼	클래스 천개로 나눠요
@김젼	요게 세개정도
@김젼	나머진 저녁에 해야될거같고
@김젼	오늘시간엔
@김젼	심플 뉴럴넷에서
@김젼	퍼셉트론이
@김젼	어떻게 학습을 하느냐를
@김젼	보여드릴게요
@김젼	뉴럴넷을
@김젼	그릴떄
@김젼	차원이 딱 2차원이라고 해봐요
@김젼	x를 x1, x2로 인코딩해
@김젼	그럼
@Nemo	사실 퍼셉트론이나 로지스틱 리그레션이나..
@김젼	y = w0 + w1x1+ w2x2 죠?
@김젼	점이 이렇게 두다다닥 있으면
@김젼	얘를 나누는 선이
@김젼	이렇게 하나만 쓱 그어지는거에요
@김젼	w0는 y절편이에요
@김젼	bias term
@김젼	y = for i in 0..n { w_i x_i }
@김젼	x_0 = 1
@김젼	나중에
@김젼	그냥 대충
@김젼	sigma_i w_i x_i
@김젼	이렇게 쓸거니까
@김젼	혼동하지 마세요
@김젼	이건 또 사실
@김젼	w벡터와 x 벡터의
@김젼	내적이죠
@김젼	f(x, w) = dot(w, x)
@김젼	또는w
@김젼	transpose(w) * x
@김젼	그럼
@김젼	선형대수에서 다 한건데
@김젼	뉴럴넷이 뭐가 신기하냐
@김젼	이럴텐데
@김젼	이터레이티브하게
@김젼	웨이트를 계~속 업데이트할 수 있어
@김젼	데이터가
@김젼	처음부터 다 들어오지않고
@김젼	한개씩 들어와요
@김젼	뉴메리컬 어프록시메이션 해야돼요
@김젼	이너 프로덕트 계산하는.
@Nemo	전문용어로 Online learning이라고 부름
@김젼	이게
@김젼	리니어 유닛인데
@김젼	SIgmoid Unit
@김젼	뉴런이
@김젼	바이올로지 용어잖아?
@김젼	바이올로지스트들이
@김젼	저희가 이걸 뉴런이라고 부르는걸 보면
@김젼	굉장히 기분나빠할거에요.
@김젼	실제 뉴런은 저렇게 간단하지 않아
@Nemo	sigmoid function : f(x) = 1 / (1 + e**(-x))
@김젼	유닛이라고 부릅시다
@김젼	우왕
@김젼	쩐당
@김젼	w 어디써
@김젼	ㅇㅅㅇ
@김젼	sigmoid unit
@Nemo	x가 wTx
@Nemo	말하는거였어
@김젼	f(x, w) = \sigma ( sigma_i w_i x_i )
@김젼	      = \sigma (-sum)
@Nemo	그러면 우리가 예측한 직선위에
@Nemo	점이 있으면
@Nemo	f(x)가 0.5고
@Nemo	0쪽에 가까울수록 0 vice versa
<--	Joke (uid5382@highgate.irccloud.com) has quit (Ping timeout: 260 seconds)
@김젼	sigmoid는 S자 모양이에요
@김젼	ㅇㅎ
@김젼	오훙훙
@김젼	리니어 유닛이 그냥
@김젼	y = kx + n
@김젼	꼴이라면
@김젼	시그모이드 유닛은
@Nemo	y = sigmoid(kx_n)
@Nemo	kx+n
@김젼	x-> INF 면 y -> 1
@김젼	f(x) = 0.5
@김젼	x -> -INF 면 y = 0
@김젼	28년 걸려서
@김젼	퍼셉트론이
@Nemo	f'(x) = f(x)(1-f(x))
@김젼	 MLP로 온게
@김젼	시그모이드가 혁혁한 공을 했어요
@김젼	층을 쌓아도
@김젼	모델이 복잡하지 않으면 의미가 없는데
@김젼	리니어 유닛가지고
@김젼	층을 암만 쌓아봤자
-->	Joke (uid5382@highgate.irccloud.com) has joined #hyeon
@김젼	다 합치면 결국 하나의 합쳐진 리니어함수가 나와요.
@김젼	근데
@김젼	시그모이드는
@김젼	넌리니어하고 연속이고 미분가능해요
@Nemo	미분 얘기 지난시간에 내가 잠깐 언급했지?
@김젼	ㅇㅇ
@김젼	미분해서
@김젼	델타가 0인애에 가까워질수록
@김젼	학습을 하는거라고
@김젼	했던거같은
@김젼	아마 실제로 코딩하기전까진 이해 못할것같기도 하지마
@김젼	관악산을 올라가서
@김젼	어느길로 내려가면
@김젼	가장 빠를까
@김젼	이런문제에요.
@김젼	dsigma / dsum = sigma(1-sigma)
@김젼	학습하고자 한다는건
@김젼	결국
@김젼	w 라는 파라미터를 찾고싶어
@김젼	w에 뭔가 dw 를 더해서
@김젼	현재 값을 업데이트한다고 생각해보
@김젼	w_new = w_old + dw
@김젼	데이터 하나 넣어보고
@김젼	업데이트하고
@김젼	데이터 하나 보고
@김젼	업데이트하고
@김젼	이렇게
@김젼	최적의 w 를 찾아가
@김젼	지금 널 데리러가
@김젼	질문있나요
@김젼	그래서
@김젼	그거를
@김젼	이걸 델타 룰이라고 하는데
@김젼	델타
@김젼	학습규칙이에요
@김젼	델타룰
@김젼	더하기
@김젼	샘플 퍼셉트론을 학습하는
@김젼	그냥 퍼셉트론 내지는 simple perceptron이라고 하는데
@김젼	%s/샘플/심플/gc
@김젼	Delta Rule
@김젼	Given : D = X(d), y\
@김젼	Given : D = {(X(d), y(d))} d=1~N
@김젼	이미지 인식이었다면
@김젼	100 * 100 이미지면
@김젼	N = 10000이고
@김젼	x, y는 각각의 값이되는거에요
@김젼	f(x, w) = sigma w_i x_i
@김젼	Find : w*
@김젼	아
@김젼	하필이면
@김젼	밤샌날 다음 수업이
@김젼	이렇게 재밌는 내용이
@김젼	시부엉
@Nemo	밤샘? ㄷㄷ
@김젼	리니어 유닛이면
@김젼	f(x, w) = Sigma_i w_i * x_i
@김젼	에러는
@김젼	E = (y(d) - f(x(d), w))^2
@김젼	여기서 학습은
@김젼	Find : w start <- argmin_w E
@김젼	ㅇㅅㅇ
@김젼	Find : w star <- argmin_w E
@김젼	이건
@김젼	Batch learning이에요
@Nemo	이것두 지난번에 내가 언급했을듯
@김젼	그랬던거같음
@Nemo	히히
@김젼	역시 코딩해보기 전까진 이해못할거같지만
@김젼	w* 는
@김젼	에러중에 w 만 관심있어
@김젼	w를 바꿔가면서
@김젼	E_N(w)가
@김젼	최소가 되는
@김젼	w를 찾는거에요
@김젼	wn이 양수면
@김젼	익사이토릭 뉴런이라고 하고
@Nemo	자극 강화 억제 시냅스
@김젼	wn이 음수면
@김젼	인히비토리 뉴런이락 해요
@김젼	두가지가 같이 있는데
@김젼	심플 퍼셉트론
@김젼	뉴럴 네트워크다!
@김젼	>_<
@김젼	그래서
@김젼	(니모) 그래서
@김젼	통계의 리그레션이랑
@김젼	심플 퍼셉트론이랑
@김젼	별 차이 없다고 보는건가요?
@김젼	그래요
@김젼	그냥
@김젼	수학적으론 거의 같은거에요
@김젼	이름을 다르게 불렀어
@김젼	그런것들도 다
@김젼	유연성이에ㅛ
@김젼	요
@김젼	물론
@김젼	다 똑같지 않을 수 있는게 있는데
@김젼	학문 분야마다
@김젼	같은걸 다르게 불러
@김젼	요거죠>
@김젼	요거를
@Nemo	그래서 뭐가 다르다는거지
@Nemo	모르겠다
@김젼	그래서 학습이 뭐라그랬냐면
@김젼	지난번에 잠깐 얘기했는데
@김젼	학습을 모로 정의하냐면은
@김젼	에라를
@김젼	w에 대해서
@김젼	미분을 해봐가지고
@김젼	*편미분을 해가지고
@김젼	이 값이 0이 되는
@김젼	round E(w)/round w
@김젼	를 0으로 보내는
@김젼	w를 찾는거에요
@김젼	더 좋은거
@김젼	더 좋은건
@김젼	저기 멀리 있을 수 있는데
@김젼	우리가 쓰는건
@김젼	로컬 미니마에요
@김젼	제일 근처에 있는
@김젼	도함수가 0이 되는 지점을 찾는 알고리즘이에요
@Nemo	나중에 얘기할수 있을까
@김젼	이거 극복할라면
@Nemo	흐흐
@김젼	자꾸 랜덤 점프를 해야돼
@김젼	머신러닝이 그래서
@Nemo	그래서 스토캐스틱.. 이것도 내가 말했지롱
@김젼	컴퓨팅 파워를
@김젼	많이 필요로 해
@김젼	계속 반복하니까
@김젼	학습한다는거 ㄴ겨룩ㄱ 그래ㅓㅅ
@김젼	에러에 대해서
@김젼	그래서
@김젼	지금
@김젼	심플 퍼셉트론에 대해
@김젼	저 무식한 식을
@김젼	정말 계산하는건가?!
@Nemo	미분한거 생각보다 깔쌈하게 나와
@김젼	Online learning에 대해
@김젼	먼저 할게요
@김젼	데이터 하나가 들어왔을떄
@김젼	w_i가
@김젼	어떻게 변하나
@김젼	봅시다!
@김젼	\round Ed / \round w_i =
@김젼	round/round w_i (yd - fd)^2
@김젼	자 그래서 미분하면 어떻게되죠
@김젼	= 2(yd - fd) round / round dwi (yd - fd()
@김젼	졸려서
@김젼	계속 움직였더니
@김젼	더워
@김젼	(....)
@김젼	빨리
@김젼	미분 결과를 달라
@김젼	wi에 대한 편미분이니
@김젼	wi가 아닌 항은
@김젼	날려버려요!
@김젼	round Ed / round wi = -2(yd- fd)xi^d
@김젼	wi <- wi + mu round E/round W
@Nemo	뉴럴넷도 결국 이거 쓰면 끝
@김젼	Depta Rule for Linear Unit
@김젼	w_i = w_i + \mu (yd - fd) xi**4
@김젼	w_i <- w_i + \mu (yd - fd) xi**4
@김젼	현재 웨이트가 있는데
@김젼	새로운 웨이트로 바꾸는게
@김젼	핵심이라그랬죠?
@김젼	필기
@김젼	잘못
@김젼	w_i <- w_i + \mu (yd - fd) xi**d
@김젼	**도 아니다
@김젼	w_i <- w_i + \mu (yd - fd) xi_d
@김젼	w_i <- w_i + \mu (y_d - f_d) xi_d
@김젼	y_d가 타깃값이고
@김젼	f_d 가 액츄어
@김젼	액츙러
@김젼	타입이랑액츄얼을 같게 만드는게 목표
@Nemo	칼국수먹고싶다
@Nemo	아 이걸 왜 쳤지
@Nemo	....
@sgm	\mu
@김젼	백 프로퍼게이션
@sgm	\eta
@김젼	알고리즘은
@김젼	이걸 일반화시킨거에요
@김젼	이건
@김젼	관측되는 출력에 대해서만 한건데
@김젼	이제 히든에
@김젼	대해서도 할거에요
@김젼	그거밖에 차이가 없어요
@Nemo	정말 폭풍진도다
@김젼	더워
@김젼	저거
@김젼	뮤가 아니라
@김젼	에타구나
@김젼	쏘 핫한
@김젼	머신러닝
@김젼	에타를
@김젼	키우면
@김젼	한번
@김젼	학습할때
@김젼	스트라이드가 길어요
@김젼	쩜프를 막 많이해
@김젼	이롡적으로는
@김젼	작게하는게 맞는데
@김젼	크게하는게 좋은경우도 있고
@김젼	이 에타를
@김젼	Learning Rate라고 하고
@김젼	다른말로
@김젼	스텝사이즈라고도 해요
@김젼	한스테에 얼마씩 갈거냐
@김젼	에타가지금
@김젼	그래디언트에다가
@김젼	앞에 붙었으니까
@김젼	음수가 되야하는건가요?
@김젼	그래서
@김젼	저 식에대해서는
@김젼	플러스가 되는건가요?
@김젼	이론적으로는
@김젼	배치가 맞는데
@김젼	프랙티컬하게는
@김젼	온라인을 썽
@김젼	wi에
@김젼	그래디언트 E를
@김젼	더해줌
@김젼	그래디언트 E = - 에타 (라운드 E / 라운드 w)
```
